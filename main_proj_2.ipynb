{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas by my house hit 339 im going to chapel hill on sat  üòÇ', 'theo walcott is still shit fan watch rafa and johnny deal with him on saturday', 'its not that im a gsp fan i just hate nick diaz cant wait for february', 'iranian general says israels iron dome cant deal with their missiles keep talking like that and we may end up finding out', 'tehran mon amour obama tried to establish ties with the mullahs  via no barack obama  vote mitt romney', 'i sat through this whole movie just for harry and ron at christmas ohlawd 911', 'mashed out to niggas in paris in the club while in paris as cliche as it may sound weouthere', 'larry bird is ranked 4th alltime not including lebron or kobe just sayin']\n",
      "['arianagrande ari by ariana grande 80 full singer actress', 'ariana grande kiis fm yours truly cd listening party in burbank arianagrande', 'ariana grande white house easter egg roll in washington arianagrande', 'cd musics ariana grande sweet like candy 34 oz 100 ml sealed in box 100 authenic new', 'side to side   sidetoside arianagrande musically comunidadgay lgbt  lotb ', 'hairspray live previews at the macys thanksgiving day parade arianagrande televisionnbc', 'lindsaylohan is feeling thankful after blasting arianagrande for wearing toomuch ', 'i hate her but i love her songs dammit arianagrande', 'ariana grande  right there ft big sean   arianagrande', 'which one would you prefer to listen to for a whole day   i could never choose arianagrande intoyou sidetoside songs poll', 'booty baby ariarianagrande princessari bootybaby dangerouswomantour dangerouswoman']\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "##### STRIP TWEET #####################################\n",
    "#######################################################\n",
    "\n",
    "#import pandas as pd\n",
    "#df = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "#df[2].tolist() #sentiments\n",
    "#corpus = df[3].tolist()\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "#READ TRAINING SET\n",
    "df = pd.read_csv(\"small_train.tsv\", sep='\\t', header=None)\n",
    "#df[2].tolist() #sentiments\n",
    "train_corpus = df[3].tolist()\n",
    "\n",
    "#READ TEST SET\n",
    "df = pd.read_csv(\"small_test.tsv\", sep='\\t', header=None)\n",
    "#df[2].tolist() #sentiments\n",
    "test_corpus = df[3].tolist()\n",
    "\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)   \n",
    "\n",
    "def clean_corpus(corpus):\n",
    "    clean_corpus = []\n",
    "    for tweet in corpus:\n",
    "        tweet = re.sub(r\"http\\S+\", \"\", tweet) #remove link\n",
    "        tweet = ' '.join([word for word in tweet.split(' ')  if not word.startswith('@')])\n",
    "        tweet = tweet.translate(translate_table) #remove symbols \n",
    "        tweet = tweet.lower()\n",
    "        clean_corpus.append(tweet)\n",
    "    return clean_corpus\n",
    "    \n",
    "clean_train_corpus = clean_corpus(train_corpus)\n",
    "clean_test_corpus = clean_corpus(test_corpus)\n",
    "\n",
    "print(clean_train_corpus)\n",
    "print(clean_test_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gas', 'by', 'my', 'house', 'hit', '339', 'im', 'going', 'to', 'chapel', 'hill', 'on', 'sat', 'üòÇ'], ['theo', 'walcott', 'is', 'still', 'shit', 'fan', 'watch', 'rafa', 'and', 'johnny', 'deal', 'with', 'him', 'on', 'saturday'], ['its', 'not', 'that', 'im', 'a', 'gsp', 'fan', 'i', 'just', 'hate', 'nick', 'diaz', 'cant', 'wait', 'for', 'february'], ['iranian', 'general', 'says', 'israels', 'iron', 'dome', 'cant', 'deal', 'with', 'their', 'missiles', 'keep', 'talking', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'finding', 'out'], ['tehran', 'mon', 'amour', 'obama', 'tried', 'to', 'establish', 'ties', 'with', 'the', 'mullahs', 'via', 'no', 'barack', 'obama', 'vote', 'mitt', 'romney'], ['i', 'sat', 'through', 'this', 'whole', 'movie', 'just', 'for', 'harry', 'and', 'ron', 'at', 'christmas', 'ohlawd', '911'], ['mashed', 'out', 'to', 'niggas', 'in', 'paris', 'in', 'the', 'club', 'while', 'in', 'paris', 'as', 'cliche', 'as', 'it', 'may', 'sound', 'weouthere'], ['larry', 'bird', 'is', 'ranked', '4th', 'alltime', 'not', 'including', 'lebron', 'or', 'kobe', 'just', 'sayin']]\n",
      "[['arianagrande', 'ari', 'by', 'ariana', 'grande', '80', 'full', 'singer', 'actress'], ['ariana', 'grande', 'kiis', 'fm', 'yours', 'truly', 'cd', 'listening', 'party', 'in', 'burbank', 'arianagrande'], ['ariana', 'grande', 'white', 'house', 'easter', 'egg', 'roll', 'in', 'washington', 'arianagrande'], ['cd', 'musics', 'ariana', 'grande', 'sweet', 'like', 'candy', '34', 'oz', '100', 'ml', 'sealed', 'in', 'box', '100', 'authenic', 'new'], ['side', 'to', 'side', 'sidetoside', 'arianagrande', 'musically', 'comunidadgay', 'lgbt', 'lotb'], ['hairspray', 'live', 'previews', 'at', 'the', 'macys', 'thanksgiving', 'day', 'parade', 'arianagrande', 'televisionnbc'], ['lindsaylohan', 'is', 'feeling', 'thankful', 'after', 'blasting', 'arianagrande', 'for', 'wearing', 'toomuch'], ['i', 'hate', 'her', 'but', 'i', 'love', 'her', 'songs', 'dammit', 'arianagrande'], ['ariana', 'grande', 'right', 'there', 'ft', 'big', 'sean', 'arianagrande'], ['which', 'one', 'would', 'you', 'prefer', 'to', 'listen', 'to', 'for', 'a', 'whole', 'day', 'i', 'could', 'never', 'choose', 'arianagrande', 'intoyou', 'sidetoside', 'songs', 'poll'], ['booty', 'baby', 'ariarianagrande', 'princessari', 'bootybaby', 'dangerouswomantour', 'dangerouswoman']]\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "##### TOKENIZATION ####################################\n",
    "#######################################################\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def tokenize(clean_corpus):\n",
    "    tokens = []\n",
    "    for tweet in clean_corpus:\n",
    "        token = []\n",
    "        token = word_tokenize(tweet)\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "train_tokens = tokenize(clean_train_corpus)\n",
    "test_tokens = tokenize(clean_test_corpus)\n",
    "\n",
    "    \n",
    "print(train_tokens)\n",
    "print(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gas', 'by', 'my', 'house', 'hit', '339', 'im', 'go', 'to', 'chapel', 'hill', 'on', 'sat', 'üòÇ'], ['theo', 'walcott', 'be', 'still', 'shit', 'fan', 'watch', 'rafa', 'and', 'johnny', 'deal', 'with', 'him', 'on', 'saturday'], ['it', 'not', 'that', 'im', 'a', 'gsp', 'fan', 'i', 'just', 'hate', 'nick', 'diaz', 'cant', 'wait', 'for', 'february'], ['iranian', 'general', 'say', 'israel', 'iron', 'dome', 'cant', 'deal', 'with', 'their', 'missile', 'keep', 'talk', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'find', 'out'], ['tehran', 'mon', 'amour', 'obama', 'try', 'to', 'establish', 'tie', 'with', 'the', 'mullah', 'via', 'no', 'barack', 'obama', 'vote', 'mitt', 'romney'], ['i', 'sat', 'through', 'this', 'whole', 'movie', 'just', 'for', 'harry', 'and', 'ron', 'at', 'christmas', 'ohlawd', '911'], ['mash', 'out', 'to', 'nigga', 'in', 'paris', 'in', 'the', 'club', 'while', 'in', 'paris', 'a', 'cliche', 'a', 'it', 'may', 'sound', 'weouthere'], ['larry', 'bird', 'be', 'ranked', '4th', 'alltime', 'not', 'include', 'lebron', 'or', 'kobe', 'just', 'sayin']]\n",
      "[['arianagrande', 'ari', 'by', 'ariana', 'grande', '80', 'full', 'singer', 'actress'], ['ariana', 'grande', 'kiis', 'fm', 'yours', 'truly', 'cd', 'listen', 'party', 'in', 'burbank', 'arianagrande'], ['ariana', 'grande', 'white', 'house', 'easter', 'egg', 'roll', 'in', 'washington', 'arianagrande'], ['cd', 'music', 'ariana', 'grande', 'sweet', 'like', 'candy', '34', 'oz', '100', 'ml', 'seal', 'in', 'box', '100', 'authenic', 'new'], ['side', 'to', 'side', 'sidetoside', 'arianagrande', 'musically', 'comunidadgay', 'lgbt', 'lotb'], ['hairspray', 'live', 'preview', 'at', 'the', 'macys', 'thanksgiving', 'day', 'parade', 'arianagrande', 'televisionnbc'], ['lindsaylohan', 'be', 'feel', 'thankful', 'after', 'blasting', 'arianagrande', 'for', 'wear', 'toomuch'], ['i', 'hate', 'her', 'but', 'i', 'love', 'her', 'song', 'dammit', 'arianagrande'], ['ariana', 'grande', 'right', 'there', 'ft', 'big', 'sean', 'arianagrande'], ['which', 'one', 'would', 'you', 'prefer', 'to', 'listen', 'to', 'for', 'a', 'whole', 'day', 'i', 'could', 'never', 'choose', 'arianagrande', 'intoyou', 'sidetoside', 'song', 'poll'], ['booty', 'baby', 'ariarianagrande', 'princessari', 'bootybaby', 'dangerouswomantour', 'dangerouswoman']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "# Lemmatize with POS Tag\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    tweets = []\n",
    "    for token_list in tokens:\n",
    "        lemmatized = []\n",
    "        for word in token_list:\n",
    "            lemmatized.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "        tweets.append(lemmatized)\n",
    "    return tweets\n",
    "    \n",
    "train_tweets = lemmatize(train_tokens)\n",
    "test_tweets = lemmatize(test_tokens)\n",
    "\n",
    "print(train_tweets)\n",
    "print(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas by my house hit 339 im go to chapel hill on sat üòÇ', 'theo walcott be still shit fan watch rafa and johnny deal with him on saturday', 'it not that im a gsp fan i just hate nick diaz cant wait for february', 'iranian general say israel iron dome cant deal with their missile keep talk like that and we may end up find out', 'tehran mon amour obama try to establish tie with the mullah via no barack obama vote mitt romney', 'i sat through this whole movie just for harry and ron at christmas ohlawd 911', 'mash out to nigga in paris in the club while in paris a cliche a it may sound weouthere', 'larry bird be ranked 4th alltime not include lebron or kobe just sayin']\n",
      "['arianagrande ari by ariana grande 80 full singer actress', 'ariana grande kiis fm yours truly cd listen party in burbank arianagrande', 'ariana grande white house easter egg roll in washington arianagrande', 'cd music ariana grande sweet like candy 34 oz 100 ml seal in box 100 authenic new', 'side to side sidetoside arianagrande musically comunidadgay lgbt lotb', 'hairspray live preview at the macys thanksgiving day parade arianagrande televisionnbc', 'lindsaylohan be feel thankful after blasting arianagrande for wear toomuch', 'i hate her but i love her song dammit arianagrande', 'ariana grande right there ft big sean arianagrande', 'which one would you prefer to listen to for a whole day i could never choose arianagrande intoyou sidetoside song poll', 'booty baby ariarianagrande princessari bootybaby dangerouswomantour dangerouswoman']\n"
     ]
    }
   ],
   "source": [
    "final_train_corpus = []\n",
    "for tweet in train_tweets:\n",
    "    final_train_corpus.append(\" \".join(str(word) for word in tweet))\n",
    "\n",
    "final_test_corpus = []\n",
    "for tweet in test_tweets:\n",
    "    final_test_corpus.append(\" \".join(str(word) for word in tweet))\n",
    "\n",
    "print(final_train_corpus)\n",
    "print(final_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAG-OF-WORDS VECTORIZATION\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_BOW_train = vectorizer.fit_transform(final_train_corpus)\n",
    "X_BOW_test = vectorizer.fit_transform(final_test_corpus)\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(X.toarray()[0])\n",
    "#print(X_BOW.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TF-IDF VECTORIZATION\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_TFIDF_train = vectorizer.fit_transform(final_train_corpus)\n",
    "X_TFIDF_test = vectorizer.fit_transform(final_test_corpus)\n",
    "#print(X_TFIDF.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.42836308e-03 -4.35149821e-04 -3.22528929e-03 -7.66599376e-04\n",
      "  6.02648070e-04 -6.81098682e-05 -3.40442732e-03  1.91850448e-03\n",
      " -2.57422635e-03 -3.06543987e-03  1.75409287e-03  9.31886607e-04\n",
      "  4.83536301e-03 -8.47664021e-04  1.31994393e-03 -3.14032251e-04\n",
      "  4.54242574e-03  3.48173990e-03 -2.57371779e-04  1.21492259e-04\n",
      " -4.25525801e-03  4.32937266e-03  2.03681714e-03 -1.00503625e-04\n",
      " -3.74775846e-03  1.57843949e-03 -8.49862292e-04 -3.47181712e-03\n",
      "  3.12479166e-03 -3.67186475e-03 -3.98839125e-03  4.71629295e-03\n",
      " -1.42485951e-03 -5.56298182e-04 -6.92732632e-04 -2.89811543e-03\n",
      "  1.50154636e-03 -1.22510153e-03  1.37926824e-03  1.05953310e-03\n",
      "  2.47655623e-03  4.97408211e-03 -3.53035191e-03 -3.19760898e-03\n",
      " -1.25667523e-03  4.64401906e-03  9.44819418e-04  3.34101007e-03\n",
      "  3.17699532e-03  2.52141990e-03  4.85878112e-03  2.23506871e-03\n",
      " -4.92161885e-03  8.73634301e-04  2.12432910e-03 -3.49960732e-03\n",
      "  4.29939898e-03  1.70160935e-03  1.36006111e-03 -8.50588258e-04\n",
      "  6.00519124e-04  2.05080747e-03 -2.69077299e-03  4.94691683e-03\n",
      "  4.66491748e-03  2.07307260e-03 -2.12231092e-03 -4.09968523e-03\n",
      " -8.93159187e-04  1.13925419e-03  4.27235197e-03 -1.15841476e-03\n",
      " -4.81228996e-03  1.52234850e-03 -2.84669641e-03 -1.26744725e-03\n",
      " -3.62229650e-03 -2.89571448e-03  3.03869694e-03 -4.39139083e-03\n",
      " -1.63910235e-03  6.96865609e-04 -2.27744482e-03  4.10768483e-03\n",
      " -9.40705824e-04  7.16398587e-04  3.52461240e-03  3.21913947e-04\n",
      "  4.30817110e-03  2.04871292e-04  2.38636800e-04  4.58184630e-03\n",
      " -7.80211471e-04  3.19449906e-03 -2.83159991e-03 -1.70740462e-03\n",
      " -4.70202183e-03  3.33598279e-03  2.58923089e-03 -4.61163977e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitra/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "#we have 3 options: word2vec, word2vec in sklearn, doc2vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "#IMPORT WORD2VEC\n",
    "from gensim.models import Word2Vec\n",
    "nested_corpus = [final_train_corpus[i:i+1] for i in range(0, len(final_train_corpus), 1)]\n",
    "model = Word2Vec(train_tokens, size=100, window=5, min_count=1, workers=4) #size of vector is 100\n",
    "model.train(train_tokens, total_examples=model.corpus_count,epochs=model.epochs)  # train word vectors\n",
    "print(model['gas'])\n",
    "\n",
    "#IMPORT WORD2VEC LIBRARY AVAILABLE IN SKLEARN\n",
    "\n",
    "#from gensim.sklearn_api import W2VTransformer\n",
    "#total = sum(tokens,[])\n",
    "#vectorizer = W2VTransformer(size=10, min_count=1, seed=1)\n",
    "#print(corpus)\n",
    "#X_embeddings = vectorizer.fit(total).transform(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitra/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/home/dimitra/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['gas', 'by', 'my', 'house', 'hit', '339', 'im', 'go', 'to', 'chapel', 'hill', 'on', 'sat', 'üòÇ'], tags=['0']), TaggedDocument(words=['theo', 'walcott', 'be', 'still', 'shit', 'fan', 'watch', 'rafa', 'and', 'johnny', 'deal', 'with', 'him', 'on', 'saturday'], tags=['1']), TaggedDocument(words=['it', 'not', 'that', 'im', 'a', 'gsp', 'fan', 'i', 'just', 'hate', 'nick', 'diaz', 'cant', 'wait', 'for', 'february'], tags=['2']), TaggedDocument(words=['iranian', 'general', 'say', 'israel', 'iron', 'dome', 'cant', 'deal', 'with', 'their', 'missile', 'keep', 'talk', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'find', 'out'], tags=['3']), TaggedDocument(words=['tehran', 'mon', 'amour', 'obama', 'try', 'to', 'establish', 'tie', 'with', 'the', 'mullah', 'via', 'no', 'barack', 'obama', 'vote', 'mitt', 'romney'], tags=['4']), TaggedDocument(words=['i', 'sat', 'through', 'this', 'whole', 'movie', 'just', 'for', 'harry', 'and', 'ron', 'at', 'christmas', 'ohlawd', '911'], tags=['5']), TaggedDocument(words=['mash', 'out', 'to', 'nigga', 'in', 'paris', 'in', 'the', 'club', 'while', 'in', 'paris', 'a', 'cliche', 'a', 'it', 'may', 'sound', 'weouthere'], tags=['6']), TaggedDocument(words=['larry', 'bird', 'be', 'ranked', '4th', 'alltime', 'not', 'include', 'lebron', 'or', 'kobe', 'just', 'sayin'], tags=['7'])]\n",
      "[TaggedDocument(words=['arianagrande', 'ari', 'by', 'ariana', 'grande', '80', 'full', 'singer', 'actress'], tags=['0']), TaggedDocument(words=['ariana', 'grande', 'kiis', 'fm', 'yours', 'truly', 'cd', 'listen', 'party', 'in', 'burbank', 'arianagrande'], tags=['1']), TaggedDocument(words=['ariana', 'grande', 'white', 'house', 'easter', 'egg', 'roll', 'in', 'washington', 'arianagrande'], tags=['2']), TaggedDocument(words=['cd', 'music', 'ariana', 'grande', 'sweet', 'like', 'candy', '34', 'oz', '100', 'ml', 'seal', 'in', 'box', '100', 'authenic', 'new'], tags=['3']), TaggedDocument(words=['side', 'to', 'side', 'sidetoside', 'arianagrande', 'musically', 'comunidadgay', 'lgbt', 'lotb'], tags=['4']), TaggedDocument(words=['hairspray', 'live', 'preview', 'at', 'the', 'macys', 'thanksgiving', 'day', 'parade', 'arianagrande', 'televisionnbc'], tags=['5']), TaggedDocument(words=['lindsaylohan', 'be', 'feel', 'thankful', 'after', 'blasting', 'arianagrande', 'for', 'wear', 'toomuch'], tags=['6']), TaggedDocument(words=['i', 'hate', 'her', 'but', 'i', 'love', 'her', 'song', 'dammit', 'arianagrande'], tags=['7']), TaggedDocument(words=['ariana', 'grande', 'right', 'there', 'ft', 'big', 'sean', 'arianagrande'], tags=['8']), TaggedDocument(words=['which', 'one', 'would', 'you', 'prefer', 'to', 'listen', 'to', 'for', 'a', 'whole', 'day', 'i', 'could', 'never', 'choose', 'arianagrande', 'intoyou', 'sidetoside', 'song', 'poll'], tags=['9']), TaggedDocument(words=['booty', 'baby', 'ariarianagrande', 'princessari', 'bootybaby', 'dangerouswomantour', 'dangerouswoman'], tags=['10'])]\n"
     ]
    }
   ],
   "source": [
    "#Doc2vec is equal with word2vec but is more appropriate for phrases (vectorize phrases instead of words). I.e \n",
    "#1.Manos leaves the office every day at 18:00 to catch his train\n",
    "#2. This season is called Fall, because leaves fall from the trees.\n",
    "#In this way we can capture the difference between the same word used in a different context. For example we now have a\n",
    "#different representation of the word ‚Äúleaves‚Äù in the above two sentences\n",
    "\n",
    "#IMPORTANT: parameteres in my case is random, we have to pay attention to select the right ones\n",
    "\n",
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def word_embeddings(final_corpus):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(final_corpus)]\n",
    "    print(tagged_data)\n",
    "    model = Doc2Vec(size=200,\n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "    model.build_vocab(tagged_data)\n",
    "    max_epoch = 20\n",
    "    for epoch in range(max_epoch):\n",
    "        model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    X_embeddings_array = model.docvecs\n",
    "    return X_embeddings_array\n",
    "\n",
    "#convert array to list\n",
    "\n",
    "X_embeddings_array_train = word_embeddings(final_train_corpus)\n",
    "X_embeddings_array_test = word_embeddings(final_test_corpus)\n",
    "\n",
    "X_embeddings_train=[]\n",
    "for i in range(len(X_embeddings_array_train)):\n",
    "    X_embeddings_train.append(X_embeddings_array_train[i].tolist())\n",
    "    \n",
    "X_embeddings_test=[]\n",
    "for i in range(len(X_embeddings_array_test)):\n",
    "    X_embeddings_test.append(X_embeddings_array_test[i].tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add characteristics to embeddings\n",
    "import pandas as pd\n",
    "import csv\n",
    "lexica =[pd.read_csv(\"lexica/affin/affin.txt\", sep='\\t', header=None),\n",
    "              pd.read_csv(\"lexica/emotweet/valence_tweet.txt\", sep='\\t', header=None),\n",
    "              pd.read_csv(\"lexica/generic/generic.txt\", sep='\\t', engine=\"python\" ,quoting=csv.QUOTE_NONE,header=None),\n",
    "              pd.read_csv(\"lexica/nrc/val.txt\", sep='\\t', engine=\"python\",quoting=csv.QUOTE_NONE, header=None),\n",
    "              pd.read_csv(\"lexica/nrctag/val.txt\", sep='\\t', header=None)]\n",
    "\n",
    "def mean_valence(tokens,X_embeddings): #mean valence of each tweet\n",
    "    for tweet in range(len(train_tokens)):   #for each tweet\n",
    "        for df in lexica:         #for each lexicon\n",
    "            if not df[df[0].isin(tokens[tweet])].empty: #search for tweets' tokens in lexicon\n",
    "                average_tweet_sentiment = df[df[0].isin(tokens[tweet])][1].mean() #get average sentiment for this tweet\n",
    "                X_embeddings[tweet].append(average_tweet_sentiment)\n",
    "            else:\n",
    "                 X_embeddings[tweet].append(0)   #if is zero should we add mean or it will affect the vector\n",
    "    return X_embeddings\n",
    "\n",
    "def min_max_valence(tokens,X_embeddings): #min and max valence of each tweet\n",
    "    for tweet in range(len(tokens)):   #for each tweet\n",
    "        for df in lexica:         #for each lexicon\n",
    "            if not df[df[0].isin(tokens[tweet])].empty: #search for tweets' tokens in lexicon\n",
    "                max_valence = df[df[0].isin(tokens[tweet])][1].max() #get average sentiment for this tweet\n",
    "                min_valence = df[df[0].isin(tokens[tweet])][1].min()\n",
    "                X_embeddings[tweet].append(max_valence)\n",
    "                X_embeddings[tweet].append(min_valence)\n",
    "            else:  # add two values to keep all the vectors the same size\n",
    "                X_embeddings[tweet].append(0)   #if is zero should we add mean or it will affect the vector\n",
    "                X_embeddings[tweet].append(0)   #if is zero should we add mean or it will affect the vector\n",
    "    return X_embeddings\n",
    "\n",
    "def mean_for_two(tokens,X_embeddings):#mean valence for each half of a tweet\n",
    "    for tweet in range(len(train_tokens)):   #for each tweet\n",
    "        entire_tweet = train_tokens[tweet]\n",
    "        first_half= entire_tweet[:len(entire_tweet)//2]\n",
    "        second_half= entire_tweet[len(entire_tweet)//2:]\n",
    "        for df in lexica:         #for each lexicon\n",
    "            if not df[df[0].isin(first_half)].empty: #search for tweets' tokens in lexicon\n",
    "                average_tweet_sentiment = df[df[0].isin(first_half)][1].mean() #get average sentiment for this tweet\n",
    "                X_embeddings[tweet].append(average_tweet_sentiment)\n",
    "            else:  # add two values to keep all the vectors the same size\n",
    "                X_embeddings[tweet].append(0)   #if is zero should we add mean or it will affect the vector\n",
    "            if not df[df[0].isin(second_half)].empty: #search for tweets' tokens in lexicon\n",
    "                average_tweet_sentiment = df[df[0].isin(second_half)][1].mean() #get average sentiment for this tweet\n",
    "                X_embeddings[tweet].append(average_tweet_sentiment)\n",
    "            else:  # add two values to keep all the vectors the same size\n",
    "                X_embeddings[tweet].append(0)   #if is zero should we add mean or it will affect the vector\n",
    "    return X_embeddings\n",
    "\n",
    "X_embeddings_train = mean_valence(train_tokens,X_embeddings_train)\n",
    "X_embeddings_test = mean_valence(test_tokens,X_embeddings_test)\n",
    "X_embeddings_train = min_max_valence(train_tokens,X_embeddings_train)\n",
    "X_embeddings_test = min_max_valence(test_tokens,X_embeddings_test)\n",
    "X_embeddings_train = mean_for_two(train_tokens,X_embeddings_train)\n",
    "X_embeddings_test = mean_for_two(test_tokens,X_embeddings_test)\n",
    "#length of each tweet               \n",
    "for tweet in range(len(train_tokens)):   #for each tweet\n",
    "    X_embeddings_train[tweet].append(len(train_tokens[tweet]))\n",
    "\n",
    "#length of each tweet               \n",
    "for tweet in range(len(test_tokens)):   #for each tweet\n",
    "    X_embeddings_test[tweet].append(len(test_tokens[tweet]))\n",
    "  \n",
    "            \n",
    "for i in range(len(X_embeddings_train)):\n",
    "    print(X_embeddings_train[i])\n",
    "\n",
    "for i in range(len(X_embeddings_train)):\n",
    "    print(X_embeddings_test[i])\n",
    "#sentiment_sum = 0\n",
    "#words_found = 0\n",
    "#for j in range(len(tokens[i])):\n",
    "#    if not df[df[0] == tokens[i][j]].empty: #search for tweets' tokens in lexicon\n",
    "#        sentiment_sum += df[df[0] == tokens[i][j]].iloc[0][1] #get sentiment\n",
    "#        words_found++\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#CONSTRUCT Y_LABELS#\n",
    "####################\n",
    "\n",
    "df = pd.read_csv(\"small.tsv\", sep='\\t', header=None)\n",
    "y_labels = df[2].tolist() #sentiments\n",
    "\n",
    "\n",
    "for n, value in enumerate(y_labels):\n",
    "    if value == \"positive\":\n",
    "        y_labels[n] = 2\n",
    "    elif value ==\"negative\":\n",
    "        y_labels[n] = 0\n",
    "    else:\n",
    "        y_labels[n] =1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
