{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas by my house hit 339 im going to chapel hill on sat  üòÇ', 'theo walcott is still shit fan watch rafa and johnny deal with him on saturday', 'its not that im a gsp fan i just hate nick diaz cant wait for february', 'iranian general says israels iron dome cant deal with their missiles keep talking like that and we may end up finding out', 'tehran mon amour obama tried to establish ties with the mullahs  via no barack obama  vote mitt romney', 'i sat through this whole movie just for harry and ron at christmas ohlawd 911', 'mashed out to niggas in paris in the club while in paris as cliche as it may sound weouthere', 'larry bird is ranked 4th alltime not including lebron or kobe just sayin']\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "##### STRIP TWEET #####################################\n",
    "#######################################################\n",
    "\n",
    "#import pandas as pd\n",
    "#df = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "#df[2].tolist() #sentiments\n",
    "#corpus = df[3].tolist()\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "df = pd.read_csv(\"small_train.tsv\", sep='\\t', header=None)\n",
    "#df[2].tolist() #sentiments\n",
    "train_corpus = df[3].tolist()\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)   \n",
    "\n",
    "clean_train_corpus = []\n",
    "for tweet in train_corpus:\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet) #remove link\n",
    "    tweet = ' '.join([word for word in tweet.split(' ')  if not word.startswith('@')])\n",
    "    tweet = tweet.translate(translate_table) #remove symbols \n",
    "    tweet = tweet.lower()\n",
    "    clean_train_corpus.append(tweet)\n",
    "\n",
    "print(clean_train_corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gas', 'by', 'my', 'house', 'hit', '339', 'im', 'going', 'to', 'chapel', 'hill', 'on', 'sat', 'üòÇ'], ['theo', 'walcott', 'is', 'still', 'shit', 'fan', 'watch', 'rafa', 'and', 'johnny', 'deal', 'with', 'him', 'on', 'saturday'], ['its', 'not', 'that', 'im', 'a', 'gsp', 'fan', 'i', 'just', 'hate', 'nick', 'diaz', 'cant', 'wait', 'for', 'february'], ['iranian', 'general', 'says', 'israels', 'iron', 'dome', 'cant', 'deal', 'with', 'their', 'missiles', 'keep', 'talking', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'finding', 'out'], ['tehran', 'mon', 'amour', 'obama', 'tried', 'to', 'establish', 'ties', 'with', 'the', 'mullahs', 'via', 'no', 'barack', 'obama', 'vote', 'mitt', 'romney'], ['i', 'sat', 'through', 'this', 'whole', 'movie', 'just', 'for', 'harry', 'and', 'ron', 'at', 'christmas', 'ohlawd', '911'], ['mashed', 'out', 'to', 'niggas', 'in', 'paris', 'in', 'the', 'club', 'while', 'in', 'paris', 'as', 'cliche', 'as', 'it', 'may', 'sound', 'weouthere'], ['larry', 'bird', 'is', 'ranked', '4th', 'alltime', 'not', 'including', 'lebron', 'or', 'kobe', 'just', 'sayin']]\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "##### TOKENIZATION ####################################\n",
    "#######################################################\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "train_tokens = []\n",
    "for tweet in clean_train_corpus:\n",
    "    token = []\n",
    "    token = word_tokenize(tweet)\n",
    "    train_tokens.append(token)\n",
    "\n",
    "print(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gas', 'by', 'my', 'house', 'hit', '339', 'im', 'go', 'to', 'chapel', 'hill', 'on', 'sat', 'üòÇ'], ['theo', 'walcott', 'be', 'still', 'shit', 'fan', 'watch', 'rafa', 'and', 'johnny', 'deal', 'with', 'him', 'on', 'saturday'], ['it', 'not', 'that', 'im', 'a', 'gsp', 'fan', 'i', 'just', 'hate', 'nick', 'diaz', 'cant', 'wait', 'for', 'february'], ['iranian', 'general', 'say', 'israel', 'iron', 'dome', 'cant', 'deal', 'with', 'their', 'missile', 'keep', 'talk', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'find', 'out'], ['tehran', 'mon', 'amour', 'obama', 'try', 'to', 'establish', 'tie', 'with', 'the', 'mullah', 'via', 'no', 'barack', 'obama', 'vote', 'mitt', 'romney'], ['i', 'sat', 'through', 'this', 'whole', 'movie', 'just', 'for', 'harry', 'and', 'ron', 'at', 'christmas', 'ohlawd', '911'], ['mash', 'out', 'to', 'nigga', 'in', 'paris', 'in', 'the', 'club', 'while', 'in', 'paris', 'a', 'cliche', 'a', 'it', 'may', 'sound', 'weouthere'], ['larry', 'bird', 'be', 'ranked', '4th', 'alltime', 'not', 'include', 'lebron', 'or', 'kobe', 'just', 'sayin']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "# Lemmatize with POS Tag\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_tweets = []\n",
    "for token_list in train_tokens:\n",
    "    lemmatized = []\n",
    "    for word in token_list:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "    train_tweets.append(lemmatized)\n",
    "\n",
    "print(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas by my house hit 339 im go to chapel hill on sat üòÇ', 'theo walcott be still shit fan watch rafa and johnny deal with him on saturday', 'it not that im a gsp fan i just hate nick diaz cant wait for february', 'iranian general say israel iron dome cant deal with their missile keep talk like that and we may end up find out', 'tehran mon amour obama try to establish tie with the mullah via no barack obama vote mitt romney', 'i sat through this whole movie just for harry and ron at christmas ohlawd 911', 'mash out to nigga in paris in the club while in paris a cliche a it may sound weouthere', 'larry bird be ranked 4th alltime not include lebron or kobe just sayin']\n"
     ]
    }
   ],
   "source": [
    "final_train_corpus = []\n",
    "for tweet in train_tweets:\n",
    "    final_train_corpus.append(\" \".join(str(word) for word in tweet))\n",
    "\n",
    "print(final_train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAG-OF-WORDS VECTORIZATION\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_BOW = vectorizer.fit_transform(final_train_corpus)\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(X.toarray()[0])\n",
    "#print(X_BOW.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TF-IDF VECTORIZATION\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_TFIDF = vectorizer.fit_transform(final_train_corpus)\n",
    "#print(X_TFIDF.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.4717736e-03 -4.9537281e-03  2.8488671e-03 -4.5573632e-03\n",
      " -3.7937020e-03 -2.9213043e-04  3.6277922e-03 -1.5129922e-03\n",
      "  9.7982993e-04 -8.8209321e-04  7.9758157e-04  2.5144566e-03\n",
      " -3.3176853e-03 -4.7022430e-03  2.0596345e-03 -2.7711764e-03\n",
      "  8.3480478e-04  4.7617457e-03 -3.5368849e-03 -9.4520374e-06\n",
      " -2.9633900e-03  2.9271308e-03 -1.9926042e-03 -8.3342427e-04\n",
      "  3.2666701e-04  1.2981131e-03 -4.3696197e-04  1.5445305e-03\n",
      " -1.7181310e-03 -5.0038104e-03 -1.5046336e-03 -3.1523132e-03\n",
      " -1.3387342e-03 -4.1482332e-03  2.0695240e-03 -1.6448352e-03\n",
      " -2.2167743e-03 -3.0591839e-03 -4.8770234e-03  4.9308681e-05\n",
      "  1.8038582e-03 -6.5516349e-04 -3.5723748e-03  1.4162593e-03\n",
      "  4.8562372e-03 -4.4696834e-03  2.5044356e-03  3.6430808e-03\n",
      "  2.2215347e-03 -3.0143620e-03 -4.2602401e-03  8.2416000e-04\n",
      "  1.8534381e-03  3.3984412e-05 -1.9368144e-03 -2.6199964e-03\n",
      "  1.1428250e-03 -3.5262217e-03  1.4874688e-03 -2.0158708e-03\n",
      "  2.6807149e-03 -3.1172248e-04  1.9078515e-03 -3.6214439e-03\n",
      "  1.3176476e-03  4.2016213e-03 -4.7020862e-04  2.6944054e-03\n",
      "  4.9820682e-03  4.1424097e-03  3.5511245e-04  2.9698375e-03\n",
      " -4.5377244e-03 -4.1367230e-03  3.8069799e-03  4.0979641e-03\n",
      " -2.0434563e-03  2.2326319e-03 -3.3895075e-03  4.9786284e-03\n",
      " -5.8310234e-04 -2.1542262e-03  7.6176872e-04  3.9392845e-03\n",
      "  1.6008690e-03  6.4472971e-04 -3.8710248e-03  3.1353564e-03\n",
      "  2.2842537e-03  2.2550578e-04  2.2988140e-03  1.5528236e-03\n",
      " -1.1074991e-03  2.9532905e-03 -4.4362065e-03 -4.1472823e-03\n",
      "  4.7496287e-03 -3.0105747e-03  4.0806741e-03 -2.8290815e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitra/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "#we have 3 options: word2vec, word2vec in sklearn, doc2vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "#IMPORT WORD2VEC\n",
    "from gensim.models import Word2Vec\n",
    "nested_corpus = [final_train_corpus[i:i+1] for i in range(0, len(final_train_corpus), 1)]\n",
    "model = Word2Vec(train_tokens, size=100, window=5, min_count=1, workers=4) #size of vector is 100\n",
    "model.train(train_tokens, total_examples=model.corpus_count,epochs=model.epochs)  # train word vectors\n",
    "print(model['gas'])\n",
    "\n",
    "#IMPORT WORD2VEC LIBRARY AVAILABLE IN SKLEARN\n",
    "\n",
    "#from gensim.sklearn_api import W2VTransformer\n",
    "#total = sum(tokens,[])\n",
    "#vectorizer = W2VTransformer(size=10, min_count=1, seed=1)\n",
    "#print(corpus)\n",
    "#X_embeddings = vectorizer.fit(total).transform(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitra/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/home/dimitra/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['gas', 'by', 'my', 'house', 'hit', '339', 'im', 'go', 'to', 'chapel', 'hill', 'on', 'sat', 'üòÇ'], tags=['0']), TaggedDocument(words=['theo', 'walcott', 'be', 'still', 'shit', 'fan', 'watch', 'rafa', 'and', 'johnny', 'deal', 'with', 'him', 'on', 'saturday'], tags=['1']), TaggedDocument(words=['it', 'not', 'that', 'im', 'a', 'gsp', 'fan', 'i', 'just', 'hate', 'nick', 'diaz', 'cant', 'wait', 'for', 'february'], tags=['2']), TaggedDocument(words=['iranian', 'general', 'say', 'israel', 'iron', 'dome', 'cant', 'deal', 'with', 'their', 'missile', 'keep', 'talk', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'find', 'out'], tags=['3']), TaggedDocument(words=['tehran', 'mon', 'amour', 'obama', 'try', 'to', 'establish', 'tie', 'with', 'the', 'mullah', 'via', 'no', 'barack', 'obama', 'vote', 'mitt', 'romney'], tags=['4']), TaggedDocument(words=['i', 'sat', 'through', 'this', 'whole', 'movie', 'just', 'for', 'harry', 'and', 'ron', 'at', 'christmas', 'ohlawd', '911'], tags=['5']), TaggedDocument(words=['mash', 'out', 'to', 'nigga', 'in', 'paris', 'in', 'the', 'club', 'while', 'in', 'paris', 'a', 'cliche', 'a', 'it', 'may', 'sound', 'weouthere'], tags=['6']), TaggedDocument(words=['larry', 'bird', 'be', 'ranked', '4th', 'alltime', 'not', 'include', 'lebron', 'or', 'kobe', 'just', 'sayin'], tags=['7'])]\n"
     ]
    }
   ],
   "source": [
    "#Doc2vec is equal with word2vec but is more appropriate for phrases (vectorize phrases instead of words). I.e \n",
    "#1.Manos leaves the office every day at 18:00 to catch his train\n",
    "#2. This season is called Fall, because leaves fall from the trees.\n",
    "#In this way we can capture the difference between the same word used in a different context. For example we now have a\n",
    "#different representation of the word ‚Äúleaves‚Äù in the above two sentences\n",
    "\n",
    "#IMPORTANT: parameteres in my case is random, we have to pay attention to select the right ones\n",
    "\n",
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(final_train_corpus)]\n",
    "print(tagged_data)\n",
    "model = Doc2Vec(size=200,\n",
    "                alpha=model.alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "max_epoch = 20\n",
    "for epoch in range(max_epoch):\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "X_embeddings_array = model.docvecs\n",
    "#convert array to list\n",
    "X_embeddings=[]\n",
    "for i in range(len(X_embeddings_array)):\n",
    "    X_embeddings.append(X_embeddings_array[i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add characteristics to embeddings\n",
    "import pandas as pd\n",
    "import csv\n",
    "lexica =[pd.read_csv(\"lexica/affin/affin.txt\", sep='\\t', header=None),\n",
    "              pd.read_csv(\"lexica/emotweet/valence_tweet.txt\", sep='\\t', header=None),\n",
    "              pd.read_csv(\"lexica/generic/generic.txt\", sep='\\t', engine=\"python\" ,quoting=csv.QUOTE_NONE,header=None),\n",
    "              pd.read_csv(\"lexica/nrc/val.txt\", sep='\\t', engine=\"python\",quoting=csv.QUOTE_NONE, header=None),\n",
    "              pd.read_csv(\"lexica/nrctag/val.txt\", sep='\\t', header=None)]\n",
    "\n",
    "#mean valence of each tweet\n",
    "for tweet in range(len(train_tokens)):   #for each tweet\n",
    "    for df in lexica:         #for each lexicon\n",
    "        if not df[df[0].isin(train_tokens[tweet])].empty: #search for tweets' tokens in lexicon\n",
    "            average_tweet_sentiment = df[df[0].isin(train_tokens[tweet])][1].mean() #get average sentiment for this tweet\n",
    "            X_embeddings[tweet].append(average_tweet_sentiment)\n",
    "        else:\n",
    "             X_embeddings[tweet].append(0)   #if is zero should we add mean or it will affect the vector\n",
    "\n",
    "#length of each tweet               \n",
    "for tweet in range(len(train_tokens)):   #for each tweet\n",
    "    X_embeddings[tweet].append(len(train_tokens[tweet]))\n",
    "\n",
    "\n",
    "    \n",
    "#min and max valence of each tweet\n",
    "for tweet in range(len(train_tokens)):   #for each tweet\n",
    "    for df in lexica:         #for each lexicon\n",
    "        if not df[df[0].isin(train_tokens[tweet])].empty: #search for tweets' tokens in lexicon\n",
    "            max_valence = df[df[0].isin(train_tokens[tweet])][1].max() #get average sentiment for this tweet\n",
    "            min_valence = df[df[0].isin(train_tokens[tweet])][1].min()\n",
    "            X_embeddings[tweet].append(max_valence)\n",
    "            X_embeddings[tweet].append(min_valence)\n",
    "        else:  # add two values to keep all the vectors the same size\n",
    "            X_embeddings[tweet].append(0)   #if is zero should we add mean or it will affect the vector\n",
    "            X_embeddings[tweet].append(0)   #if is zero should we add mean or it will affect the vector\n",
    "\n",
    "    \n",
    "#mean valence for each half of a tweet\n",
    "for tweet in range(len(train_tokens)):   #for each tweet\n",
    "    entire_tweet = tokens[tweet]\n",
    "    first_half= entire_tweet[:len(entire_tweet)//2]\n",
    "    second_half= entire_tweet[len(entire_tweet)//2:]\n",
    "    for df in lexica:         #for each lexicon\n",
    "        if not df[df[0].isin(first_half)].empty: #search for tweets' tokens in lexicon\n",
    "            average_tweet_sentiment = df[df[0].isin(first_half)][1].mean() #get average sentiment for this tweet\n",
    "            X_embeddings[tweet].append(average_tweet_sentiment)\n",
    "        else:  # add two values to keep all the vectors the same size\n",
    "            X_embeddings[tweet].append(0)   #if is zero should we add mean or it will affect the vector\n",
    "        if not df[df[0].isin(second_half)].empty: #search for tweets' tokens in lexicon\n",
    "            average_tweet_sentiment = df[df[0].isin(second_half)][1].mean() #get average sentiment for this tweet\n",
    "            X_embeddings[tweet].append(average_tweet_sentiment)\n",
    "        else:  # add two values to keep all the vectors the same size\n",
    "            X_embeddings[tweet].append(0)   #if is zero should we add mean or it will affect the vector\n",
    "        \n",
    "    \n",
    "            \n",
    "for i in range(len(X_embeddings)):\n",
    "    print(len(X_embeddings[i]))\n",
    "\n",
    "#sentiment_sum = 0\n",
    "#words_found = 0\n",
    "#for j in range(len(tokens[i])):\n",
    "#    if not df[df[0] == tokens[i][j]].empty: #search for tweets' tokens in lexicon\n",
    "#        sentiment_sum += df[df[0] == tokens[i][j]].iloc[0][1] #get sentiment\n",
    "#        words_found++\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#CONSTRUCT Y_LABELS#\n",
    "####################\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"small.tsv\", sep='\\t', header=None)\n",
    "y_labels = df[2].tolist() #sentiments\n",
    "\n",
    "\n",
    "for n, value in enumerate(y_labels):\n",
    "    if value == \"positive\":\n",
    "        y_labels[n] = 2\n",
    "    elif value ==\"negative\":\n",
    "        y_labels[n] = 0\n",
    "    else:\n",
    "        y_labels[n] =1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
