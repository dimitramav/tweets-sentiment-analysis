{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "##### STRIP TWEET #####################################\n",
    "#######################################################\n",
    "\n",
    "#import pandas as pd\n",
    "#df = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "#df[2].tolist() #sentiments\n",
    "#corpus = df[3].tolist()\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "f = open(\"small.tsv\")\n",
    "line = f.readline()\n",
    "\n",
    "tweets = []\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)   \n",
    "cnt = 1\n",
    "while line:\n",
    "    pretext = ' '.join(line.split()[3:])\n",
    "    pretext = ' '.join(word for word in pretext.split(' ') if not word.startswith('@')) #remove @user\n",
    "    pretext = re.sub(r\"http\\S+\", \"\", pretext) #remove link\n",
    "    pretext = pretext.translate(translate_table) #remove symbols \n",
    "    #remove numbers??\n",
    "    \n",
    "    tweet = pretext.lower()\n",
    "    tweets.append(tweet)\n",
    "    line = f.readline()\n",
    "    cnt += 1\n",
    "\n",
    "#print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gas', 'by', 'my', 'house', 'hit', '339', 'im', 'going', 'to', 'chapel', 'hill', 'on', 'sat', '😂'], ['theo', 'walcott', 'is', 'still', 'shit', 'fan', 'watch', 'rafa', 'and', 'johnny', 'deal', 'with', 'him', 'on', 'saturday'], ['its', 'not', 'that', 'im', 'a', 'gsp', 'fan', 'i', 'just', 'hate', 'nick', 'diaz', 'cant', 'wait', 'for', 'february'], ['iranian', 'general', 'says', 'israels', 'iron', 'dome', 'cant', 'deal', 'with', 'their', 'missiles', 'keep', 'talking', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'finding', 'out'], ['tehran', 'mon', 'amour', 'obama', 'tried', 'to', 'establish', 'ties', 'with', 'the', 'mullahs', 'via', 'no', 'barack', 'obama', 'vote', 'mitt', 'romney'], ['i', 'sat', 'through', 'this', 'whole', 'movie', 'just', 'for', 'harry', 'and', 'ron', 'at', 'christmas', 'ohlawd', '911'], ['mashed', 'out', 'to', 'niggas', 'in', 'paris', 'in', 'the', 'club', 'while', 'in', 'paris', 'as', 'cliche', 'as', 'it', 'may', 'sound', 'weouthere'], ['larry', 'bird', 'is', 'ranked', '4th', 'alltime', 'not', 'including', 'lebron', 'or', 'kobe', 'just', 'sayin']]\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "##### TOKENIZATION ####################################\n",
    "#######################################################\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = []\n",
    "for tweet in tweets:\n",
    "    token = []\n",
    "    token = word_tokenize(tweet)\n",
    "    tokens.append(token)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "# Lemmatize with POS Tag\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tweets = []\n",
    "for token_list in tokens:\n",
    "    lemmatized = []\n",
    "    for word in token_list:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "    tweets.append(lemmatized)\n",
    "\n",
    "#Rprint(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas by my house hit 339 im go to chapel hill on sat 😂', 'theo walcott be still shit watch rafa and johnny deal with him on saturday', 'it not that im a gsp fan i just hate nick diaz cant wait for february', 'iranian general say israel iron dome cant deal with their missile keep talk like that and we may end up find out', 'tehran mon amour obama try to establish tie with the mullah via no barack obama vote mitt romney', 'i sat through this whole movie just for harry and ron at christmas ohlawd 911', 'mash out to nigga in paris in the club while in paris a cliche a it may sound weouthere', 'larry bird be ranked 4th alltime not include lebron or kobe just sayin']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for tweet in tweets:\n",
    "    corpus.append(\" \".join(str(word) for word in tweet))\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAG-OF-WORDS VECTORIZATION\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_BOW = vectorizer.fit_transform(corpus)\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(X.toarray()[0])\n",
    "#print(X_BOW.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-30af1c28e3b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_TFIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(X_TFIDF.toarray())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "#TF-IDF VECTORIZATION\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_TFIDF = vectorizer.fit_transform(corpus)\n",
    "#print(X_TFIDF.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gas by my house hit 339 im go to chapel hill on sat 😂'], ['theo walcott be still shit watch rafa and johnny deal with him on saturday'], ['it not that im a gsp fan i just hate nick diaz cant wait for february'], ['iranian general say israel iron dome cant deal with their missile keep talk like that and we may end up find out'], ['tehran mon amour obama try to establish tie with the mullah via no barack obama vote mitt romney'], ['i sat through this whole movie just for harry and ron at christmas ohlawd 911'], ['mash out to nigga in paris in the club while in paris a cliche a it may sound weouthere'], ['larry bird be ranked 4th alltime not include lebron or kobe just sayin']]\n",
      "[ 0.00118727 -0.00420469  0.00284674  0.00052224  0.00062534 -0.00345467\n",
      " -0.00357037 -0.00089545  0.00340331 -0.00465742  0.00050744 -0.00355314\n",
      "  0.00107752  0.004303   -0.0027083  -0.0046858  -0.00407666  0.0037294\n",
      " -0.00420923  0.00044863 -0.0049019  -0.0017915  -0.0006112   0.00346838\n",
      "  0.00419763 -0.00236167  0.00456637 -0.00090893 -0.00118326 -0.0026358\n",
      "  0.00404753  0.0012394  -0.00490833  0.00280024 -0.00289248  0.00203826\n",
      "  0.00063784  0.00339936 -0.0042677   0.00389917  0.00199855  0.00071982\n",
      " -0.00157183  0.00113839 -0.00249976  0.00380171  0.00188846  0.00388827\n",
      " -0.00459712  0.00214532  0.0032862   0.00232965 -0.00274529  0.00453399\n",
      " -0.00195295 -0.00228492  0.00426173  0.00259263  0.00107051  0.00145112\n",
      "  0.00137971 -0.00313759  0.00387949 -0.00223832 -0.00252093 -0.00391932\n",
      " -0.00132398 -0.00034612  0.00499417  0.00122463 -0.00311186  0.00242391\n",
      " -0.00184979 -0.00447726 -0.00101954 -0.00043641  0.00269002  0.00051521\n",
      "  0.00456522  0.00451696 -0.00282499  0.00014245 -0.00378244 -0.00184042\n",
      " -0.00041598 -0.00378296 -0.0023981  -0.00051342 -0.00150106 -0.00349227\n",
      "  0.00011028 -0.00424445 -0.00369805 -0.00478363 -0.00112612 -0.00340716\n",
      "  0.00180901  0.00231883  0.00039961 -0.00267835]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitra/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#we have 3 options: word2vec, word2vec in sklearn, doc2vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "#IMPORT WORD2VEC\n",
    "from gensim.models import Word2Vec\n",
    "nested_corpus = [corpus[i:i+1] for i in range(0, len(corpus), 1)]\n",
    "model = Word2Vec(tokens, size=100, window=5, min_count=1, workers=4) #size of vector is 100\n",
    "model.train(tokens, total_examples=model.corpus_count,epochs=model.epochs)  # train word vectors\n",
    "print(model['gas'])\n",
    "\n",
    "#IMPORT WORD2VEC LIBRARY AVAILABLE IN SKLEARN\n",
    "\n",
    "#from gensim.sklearn_api import W2VTransformer\n",
    "#total = sum(tokens,[])\n",
    "#vectorizer = W2VTransformer(size=10, min_count=1, seed=1)\n",
    "#print(corpus)\n",
    "#X_embeddings = vectorizer.fit(total).transform(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['gas', 'by', 'my', 'house', 'hit', '339', 'im', 'go', 'to', 'chapel', 'hill', 'on', 'sat', '😂'], tags=['0']), TaggedDocument(words=['theo', 'walcott', 'be', 'still', 'shit', 'watch', 'rafa', 'and', 'johnny', 'deal', 'with', 'him', 'on', 'saturday'], tags=['1']), TaggedDocument(words=['it', 'not', 'that', 'im', 'a', 'gsp', 'fan', 'i', 'just', 'hate', 'nick', 'diaz', 'cant', 'wait', 'for', 'february'], tags=['2']), TaggedDocument(words=['iranian', 'general', 'say', 'israel', 'iron', 'dome', 'cant', 'deal', 'with', 'their', 'missile', 'keep', 'talk', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'find', 'out'], tags=['3']), TaggedDocument(words=['tehran', 'mon', 'amour', 'obama', 'try', 'to', 'establish', 'tie', 'with', 'the', 'mullah', 'via', 'no', 'barack', 'obama', 'vote', 'mitt', 'romney'], tags=['4']), TaggedDocument(words=['i', 'sat', 'through', 'this', 'whole', 'movie', 'just', 'for', 'harry', 'and', 'ron', 'at', 'christmas', 'ohlawd', '911'], tags=['5']), TaggedDocument(words=['mash', 'out', 'to', 'nigga', 'in', 'paris', 'in', 'the', 'club', 'while', 'in', 'paris', 'a', 'cliche', 'a', 'it', 'may', 'sound', 'weouthere'], tags=['6']), TaggedDocument(words=['larry', 'bird', 'be', 'ranked', '4th', 'alltime', 'not', 'include', 'lebron', 'or', 'kobe', 'just', 'sayin'], tags=['7'])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitra/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    }
   ],
   "source": [
    "#Doc2vec is equal with word2vec but is more appropriate for phrases (vectorize phrases instead of words). I.e \n",
    "#1.Manos leaves the office every day at 18:00 to catch his train\n",
    "#2. This season is called Fall, because leaves fall from the trees.\n",
    "#In this way we can capture the difference between the same word used in a different context. For example we now have a\n",
    "#different representation of the word “leaves” in the above two sentences\n",
    "\n",
    "#IMPORTANT: parameteres in my case is random, we have to pay attention to select the right ones\n",
    "\n",
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(corpus)]\n",
    "print(tagged_data)\n",
    "model = Doc2Vec(size=200,\n",
    "                alpha=model.alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "max_epoch = 20\n",
    "for epoch in range(max_epoch):\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "X_embeddings_array = model.docvecs\n",
    "#convert array to list\n",
    "X_embeddings=[]\n",
    "for i in range(len(X_embeddings_array)):\n",
    "    X_embeddings.append(X_embeddings_array[i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-197-a7ddebeddf8f>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-197-a7ddebeddf8f>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#Add characteristics to embeddings\n",
    "import pandas as pd\n",
    "import csv\n",
    "lexica =[pd.read_csv(\"lexica/affin/affin.txt\", sep='\\t', header=None),\n",
    "              pd.read_csv(\"lexica/emotweet/valence_tweet.txt\", sep='\\t', header=None),\n",
    "              #pd.read_csv(\"lexica/generic/generic.txt\", sep='\\t', engine=\"python\" ,header=None),\n",
    "              pd.read_csv(\"lexica/nrc/val.txt\", sep='\\t', engine=\"python\",quoting=csv.QUOTE_NONE, header=None),\n",
    "              pd.read_csv(\"lexica/nrctag/val.txt\", sep='\\t', header=None)]\n",
    "\n",
    "\n",
    "for i in range(len(tokens)):   #for each tweet\n",
    "    for df in lexica:         #for each lexicon\n",
    "        sentiment_average = 0\n",
    "        if not df[df[0].isin(tokens[i])].empty: #search for tweets' tokens in lexicon\n",
    "            average_tweet_sentiment = df[df[0].isin(tokens[i])][1].mean() #get average sentiment for this tweet\n",
    "            X_embeddings[i].append(average_tweet_sentiment)\n",
    "\n",
    "for i in range(len(X_embeddings)):\n",
    "    print(len(X_embeddings[i])\n",
    "#sentiment_sum = 0\n",
    "#words_found = 0\n",
    "#for j in range(len(tokens[i])):\n",
    "#    if not df[df[0] == tokens[i][j]].empty: #search for tweets' tokens in lexicon\n",
    "#        sentiment_sum += df[df[0] == tokens[i][j]].iloc[0][1] #get sentiment\n",
    "#        words_found++\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
