{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Ανάλυση Συναισθημάτων σε Tweets - Τεχνικές Εξόρυξης Δεδομένων Εαρινό Εξάμηνο 2019</center>\n",
    "### <center>Δήμητρα Μαυροφοράκη 1115201400104 - Ιωάννης Χαραμής 1115201400220</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Εισαγωγή\n",
    "Η άσκηση αφορά τη διαδικασία κατηγοριοποίησης tweets, σε τρεις κατηγορίες συναισθημάτων (θετικό, ουδέτερο, αρνητικό).\n",
    "\n",
    "Το πρώτο βήμα της διαδικασίας είναι η \"εκκαθάριση\" των δεδομένων, τόσο στο train set, όσο και στο test set.\n",
    "\n",
    "Στη συνέχεια, προχωράμε στο vectorization των tweets, ώστε να τα ταξινομήσουμε σύμφωνα με το συναίσθημα που προκαλούν."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip tweets\n",
    "Για το καθαρισμό των δεδομένων, χρησιμοποιούμε τη συνάρτηση clean_corpus(), η οποία αφαιρεί τα links, τα σύμβολα, όπως σημεία στίξης, '@' και '#', και μετατρέπει σε πεζούς όλους τους χαρακτήρες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import clean_corpus\n",
    "import pandas as pd\n",
    "\n",
    "#READ TRAINING SET\n",
    "df = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "train_corpus = df[3].tolist()\n",
    "\n",
    "#READ TEST SET\n",
    "df = pd.read_csv(\"test2017.tsv\", sep='\\t', header=None)\n",
    "test_corpus = df[3].tolist()\n",
    "\n",
    "clean_train_corpus = clean_corpus(train_corpus)\n",
    "clean_test_corpus = clean_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization - Lemmatization\n",
    "Με τη χρήση της tokenize() χωρίζουμε τα tweets σε tokens και ύστερα με τη lemmatize(), για κάθε token/λέξη, βρίσκουμε την αντίστοιχη ρίζα της. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tokenize, lemmatize\n",
    "\n",
    "train_tokens = tokenize(clean_train_corpus)\n",
    "test_tokens = tokenize(clean_test_corpus)\n",
    "\n",
    "train_tweets = lemmatize(train_tokens)\n",
    "test_tweets = lemmatize(test_tokens)\n",
    "\n",
    "final_train_corpus = [\" \".join(str(word) for word in tweet) for tweet in train_tweets]\n",
    "final_test_corpus = [\" \".join(str(word) for word in tweet) for tweet in test_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud\n",
    "Οι συνηθέστερες λέξεις στα \"θετικά\" tweets, είναι λέξεις κυρίως συναισθηματικά ουδέτερες όπως \"tomorrow\", \"day\", \"get\", αλλά και λέξεις σαφώς θετικά φορτισμένες όπως \"love\", \"happy\", \"excite\", \"birthday\", \"hope\" και \"good\".\n",
    "\n",
    "Αντίστοιχα, στα \"αρνητικά\" tweets υπερτερούν συναισθηματικά ουδέτερες λέξεις, όμως συναντάμε λέξεις με αρνητική υπόσταση, μεταξύ άλλων οι \"don't\", \"can't\", \"fuck\", \"shit\", \"sad\", \"hate\", \"bad\" και \"lose\".\n",
    "\n",
    "Στα \"ουδέτερα\" tweets συναντούμε αποκλειστικά λέξεις με ουδέτερο συναισθηματικό φορτίο.\n",
    "\n",
    "Συνεπώς, οι συνηθέστερες λέξεις σε ολόκληρο το σύνολο των δεδομένων προκύπτουν από τις δημοφιλέστερες λέξεις των υποσυνόλων. Όπως αναφέραμε, επικρατέστερες είναι οι ουδέτερες λέξεις για παράδειγμα \"tommorow\", \"go\", \"day\", \"night\", \"may\", \"come\", \"get\". \n",
    "\n",
    "Η πλειοψηφία των ουδέτερων λέξεων είναι λογική, όπως και η ύπαρξή τους σε κάθε υπόσυνολο/wordcloud. Αυτό οφείλεται στο ότι στην καθημερινότητά και στο λόγο μας χρησιμοποιούμε πολλές λέξεις για να περιγράψουμε καταστάσεις και όχι συναίσθημα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from utils import clean_tweets\n",
    "\n",
    "filtered_positive = []\n",
    "filtered_neutral = []\n",
    "filtered_negative = []\n",
    "\n",
    "df_train = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "\n",
    "pos_train = clean_tweets(df_train.loc[(df_train[2] == \"positive\")].copy()[3].tolist())\n",
    "neg_train = clean_tweets(df_train.loc[ (df_train[2] == \"negative\")].copy()[3].tolist())\n",
    "neu_train = clean_tweets(df_train.loc[(df_train[2] == \"neutral\")].copy()[3].tolist())\n",
    "\n",
    "for tweet in pos_train:\n",
    "    filtered_positive += [word for word in tweet.split() if word not in stopwords.words('english')]\n",
    "for tweet in neg_train:\n",
    "    filtered_negative += [word for word in tweet.split() if word not in stopwords.words('english')]\n",
    "for tweet in neu_train:\n",
    "    filtered_neutral += [word for word in tweet.split() if word not in stopwords.words('english')]\n",
    "\n",
    "total_text = filtered_positive + filtered_neutral + filtered_negative\n",
    "\n",
    "positive_count = Counter(filtered_positive)\n",
    "neutral_count = Counter(filtered_neutral)\n",
    "negative_count = Counter(filtered_negative)    \n",
    "total_count = Counter(total_text)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(dict(positive_count))\n",
    "image = wordcloud.to_image()\n",
    "display(image)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(dict(neutral_count))\n",
    "image = wordcloud.to_image()\n",
    "display(image)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(dict(negative_count))\n",
    "image = wordcloud.to_image()\n",
    "display(image)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(dict(total_count))\n",
    "image = wordcloud.to_image()\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "Για να δημιουργήσουμε τα διανύσματα χαρακτηριστικών για κάθε tweet χρησιμοποιούμε 4 vectorizers. Επιπλέον, εμπλουτίζουμε τα παραχθέντα διανύσματα με επιπλέον χαρακτηριστικά, με τη χρήση λεξικών."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import save_to_pickle\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_BOW_train = vectorizer.fit_transform(final_train_corpus)\n",
    "save_to_pickle('X_BOW_train',X_BOW_train)\n",
    "\n",
    "X_BOW_test = vectorizer.transform(final_test_corpus)\n",
    "save_to_pickle('X_BOW_test',X_BOW_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from utils import save_to_pickle\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=30)\n",
    "\n",
    "X_TFIDF_train = vectorizer.fit_transform(final_train_corpus)\n",
    "save_to_pickle('X_TFIDF_train',X_TFIDF_train)\n",
    "\n",
    "X_TFIDF_test = vectorizer.transform(final_test_corpus)\n",
    "save_to_pickle('X_TFIDF_test',X_TFIDF_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from utils import create_word_embeddings, save_to_pickle\n",
    "\n",
    "#train_tweets\n",
    "model_train = Word2Vec(train_tweets, size=200, window=5, min_count=1, workers=4) # size of vector is 200\n",
    "model_train.train(train_tweets, total_examples=model_train.corpus_count, epochs=model_train.epochs)  # train word vectors\n",
    "\n",
    "X_W2V_embeddings_train = create_word_embeddings(train_tweets, model_train)\n",
    "save_to_pickle('X_W2V_embeddings_train',X_W2V_embeddings_train)\n",
    "\n",
    "#test_tweets\n",
    "model_test = Word2Vec(test_tweets, size=200, window=5, min_count=1, workers=4) # size of vector is 200\n",
    "model_test.train(test_tweets, total_examples=model_test.corpus_count,epochs=model_test.epochs)  # train word vectors\n",
    "\n",
    "X_W2V_embeddings_test = create_word_embeddings(test_tweets, model_test)\n",
    "save_to_pickle('X_W2V_embeddings_test',X_W2V_embeddings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings using Doc2Vec\n",
    "\n",
    "Η βιβλιοθήκη Doc2Vec είναι ισοδύναμη με την Word2Vec αλλά είναι πιο κατάλληλη για το vectorization φράσεων αντί για μεμονωμένες λέξεις. Για πάραδειγμα:\n",
    "* Manos leaves the office every day at 18:00 to catch his train.\n",
    "* This season is called Fall, because leaves fall from the trees.\n",
    "\n",
    "Μέσω των Doc2Vec διανυσμάτων είναι εμφανής η διαφορά στη χρήση της ίδιας λέξης σε διαφορετικό περιεχόμενο. Στις παραπάνω προτάσεις η λέξη \"leaves\" έχει άλλη διανυσματική απεικόνιση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_doc_embeddings, save_to_pickle\n",
    "\n",
    "X_embeddings_array_train = create_doc_embeddings(final_train_corpus)\n",
    "X_embeddings_array_test = create_doc_embeddings(final_test_corpus)\n",
    "\n",
    "X_D2V_embeddings_train = [X_embeddings_array_train[i].tolist() for i in range(len(X_embeddings_array_train))]\n",
    "save_to_pickle('X_D2V_embeddings_train',X_D2V_embeddings_train)\n",
    "\n",
    "X_D2V_embeddings_test = [X_embeddings_array_test[i].tolist() for i in range(len(X_embeddings_array_test))]\n",
    "save_to_pickle('X_D2V_embeddings_test',X_D2V_embeddings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Προσθήκη χαρακτηριστικών\n",
    "Εμπλουτίζουμε τα διανύσματα που έχουν δημιουργηθεί από τους έτοιμους vectorizers με επιπλεόν στοιχεία όπως το πλήθος των λέξεων κάθε tweet, το μέσο όρο του valence κάθε tweet, το μέγιστο και το ελάχιστο valence και τα average valences που προκύπτουν αν διασπαστεί το tweet στη μέση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_to_pickle, load_from_pickle, add_characteristics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "lexica_df = [pd.read_csv(\"lexica/affin/affin.txt\", sep='\\t', header=None),\n",
    "              pd.read_csv(\"lexica/emotweet/valence_tweet.txt\", sep='\\t', header=None),\n",
    "              pd.read_csv(\"lexica/generic/generic.txt\", sep='\\t', engine=\"python\" ,quoting=csv.QUOTE_NONE,header=None),\n",
    "              pd.read_csv(\"lexica/nrc/val.txt\", sep='\\t', engine=\"python\",quoting=csv.QUOTE_NONE, header=None),\n",
    "              pd.read_csv(\"lexica/nrctag/val.txt\", sep='\\t', header=None)]\n",
    "lexica = [df.set_index(0).T.to_dict('list') for df in lexica_df]\n",
    "\n",
    "characteristics_train = add_characteristics(lexica,train_tweets)\n",
    "characteristics_test = add_characteristics(lexica,test_tweets) \n",
    "\n",
    "X_W2V_embeddings_train = load_from_pickle('X_W2V_embeddings_train')\n",
    "X_W2Vplus_embeddings_train = np.concatenate((X_W2V_embeddings_train,characteristics_train), axis=1)\n",
    "save_to_pickle('X_W2Vplus_embeddings_train',X_W2Vplus_embeddings_train)\n",
    "\n",
    "X_W2V_embeddings_test = load_from_pickle('X_W2V_embeddings_test')\n",
    "X_W2Vplus_embeddings_test = np.concatenate((X_W2V_embeddings_test,characteristics_test), axis=1)\n",
    "save_to_pickle('X_W2Vplus_embeddings_test',X_W2Vplus_embeddings_test)\n",
    "\n",
    "X_D2V_embeddings_train = load_from_pickle('X_D2V_embeddings_train')\n",
    "X_D2Vplus_embeddings_train = np.concatenate((X_D2V_embeddings_train,characteristics_train), axis=1)\n",
    "save_to_pickle('X_D2Vplus_embeddings_train',X_D2Vplus_embeddings_train)\n",
    "\n",
    "X_D2V_embeddings_test = load_from_pickle('X_D2V_embeddings_test')\n",
    "X_D2Vplus_embeddings_test = np.concatenate((X_D2V_embeddings_test,characteristics_test), axis=1)\n",
    "save_to_pickle('X_D2Vplus_embeddings_test',X_D2Vplus_embeddings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Δημιουργία label συνόλων"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import save_to_pickle\n",
    "\n",
    "sent_map = {\"positive\":2, \"neutral\":1, \"negative\":0}\n",
    "\n",
    "df = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "y_train_labels = [sent_map[sentiment] for sentiment in df[2].tolist()] #sentiments\n",
    "save_to_pickle('y_train_labels',y_train_labels)\n",
    "\n",
    "df = pd.read_csv(\"SemEval2017_task4_subtaskA_test_english_gold.tsv\", sep='\\t', header=None)\n",
    "y_test_labels = [sent_map[sentiment] for sentiment in df[1].tolist()] #sentiments\n",
    "save_to_pickle('y_test_labels',y_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Για να ταξινομήσουμε τα tweets ως προς το συναίσθημα χρησιμοποιούμε 3 ταξινομητές."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words KNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_BOW_train')\n",
    "X_test = load_from_pickle('X_BOW_test')\n",
    "y_train_labels = load_from_pickle('y_train_labels')\n",
    "y_test_labels = load_from_pickle('y_test_labels')\n",
    "\n",
    "#BOW - KNN CLASSIFICATION\n",
    "scoreBOW_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#BOW - SVM CLASSIFICATION\n",
    "scoreBOW_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF KNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_TFIDF_train')\n",
    "X_test = load_from_pickle('X_TFIDF_test')\n",
    "y_train_labels = load_from_pickle('y_train_labels')\n",
    "y_test_labels = load_from_pickle('y_test_labels')\n",
    "\n",
    "#TFIDF - KNN CLASSIFICATION\n",
    "scoreTFIDF_KNN = knn_classification(X_TFIDF_train, X_TFIDF_test, y_train_labels, y_test_labels)\n",
    "#TFIDF - SVM CLASSIFICATION\n",
    "scoreTFIDF_SVM = svm_classification(X_TFIDF_train, X_TFIDF_test, y_train_labels, y_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec KNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_D2V_embeddings_train')\n",
    "X_test = load_from_pickle('X_D2V_embeddings_test')\n",
    "y_train_labels = load_from_pickle('y_train_labels')\n",
    "y_test_labels = load_from_pickle('y_test_labels')\n",
    "\n",
    "#DOC2VEC - KNN CLASSIFICATION\n",
    "scoreD2V_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#DOC2VEC - SVM CLASSIFICATION\n",
    "scoreD2V_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec KNN-SVM (with added characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_D2Vplus_embeddings_train')\n",
    "X_test = load_from_pickle('X_D2Vplus_embeddings_test')\n",
    "y_train_labels = load_from_pickle('y_train_labels')\n",
    "y_test_labels = load_from_pickle('y_test_labels')\n",
    "\n",
    "#DOC2VEC+features - KNN CLASSIFICATION\n",
    "scoreD2Vplus_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#DOC2VEC+features - SVM CLASSIFICATION\n",
    "scoreD2Vplus_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec KNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_W2V_embeddings_train')\n",
    "X_test = load_from_pickle('X_W2V_embeddings_test')\n",
    "y_train_labels = load_from_pickle('y_train_labels')\n",
    "y_test_labels = load_from_pickle('y_test_labels')\n",
    "\n",
    "#WORD2VEC - KNN CLASSIFICATION\n",
    "scoreW2V_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#WORD2VEC - SVM CLASSIFICATION\n",
    "scoreW2V_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec KNN-SVM (with added characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_W2Vplus_embeddings_train')\n",
    "X_test = load_from_pickle('X_W2Vplus_embeddings_test')\n",
    "y_train_labels = load_from_pickle('y_train_labels')\n",
    "y_test_labels = load_from_pickle('y_test_labels')\n",
    "\n",
    "#WORD2VEC+features - KNN CLASSIFICATION\n",
    "scoreW2Vplus_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#WORD2VEC+features - SVM CLASSIFICATION\n",
    "scoreW2Vplus_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round Robin\n",
    "\n",
    "Αρχικά, διαχωρίζουμε το trainset σε 3 ξεχωριστά υποσύνολα (positive/negative, positive/neutral, negative/neutral).\n",
    "Με τα σύνολα αυτά εκπαιδεύουμε τους Nearest Neighbor Classifiers.\n",
    "\n",
    "Στη συνέχεια, εξάγουμε τα posteriors των trainset & testset, τα οποία εισάγουμε στον KNN Classifier ως train και test data αντίστοιχα, για να αξιολογήσουμε τη μέθοδο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_posteriors, knn_classification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "train_tweets = df_train[3].tolist()\n",
    "df_test = pd.read_csv(\"test2017.tsv\", sep='\\t', header=None)\n",
    "test_tweets = df_test[3].tolist()\n",
    "\n",
    "sent_map = {\"positive\":2, \"neutral\":1, \"negative\":0}\n",
    "\n",
    "pos_neg_train = df_train.loc[(df_train[2] == \"positive\") | (df_train[2] == \"negative\")].copy()\n",
    "pos_neg_labels = [sent_map[sentiment] for sentiment in pos_neg_train[2].tolist()] #sentiments\n",
    "pos_neg_posteriors = create_posteriors(pos_neg_train[3].tolist(),train_tweets,test_tweets,pos_neg_labels,1)\n",
    "save_to_pickle('pos_neg_posteriors',pos_neg_posteriors)\n",
    "\n",
    "pos_neu_train = df_train.loc[(df_train[2] == \"positive\") | (df_train[2] == \"neutral\")].copy()\n",
    "pos_neu_labels = [sent_map[sentiment] for sentiment in pos_neu_train[2].tolist()]\n",
    "pos_neu_posteriors = create_posteriors(pos_neu_train[3].tolist(),train_tweets,test_tweets,pos_neu_labels,1)\n",
    "save_to_pickle('pos_neu_posteriors',pos_neu_posteriors)\n",
    "\n",
    "neg_neu_train = df_train.loc[(df_train[2] == \"negative\") | (df_train[2] == \"neutral\")].copy()\n",
    "neg_neu_labels = [sent_map[sentiment] for sentiment in neg_neu_train[2].tolist()]\n",
    "neg_neu_posteriors = create_posteriors(neg_neu_train[3].tolist(),train_tweets,test_tweets,neg_neu_labels,1)\n",
    "save_to_pickle('neg_neu_posteriors',neg_neu_posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import knn_classification\n",
    "\n",
    "y_train_labels = load_from_pickle('y_train_labels')\n",
    "y_test_labels = load_from_pickle('y_test_labels')\n",
    "pos_neg_posteriors = load_from_pickle('pos_neg_posteriors')\n",
    "pos_neu_posteriors = load_from_pickle('pos_neu_posteriors')\n",
    "neg_neu_posteriors = load_from_pickle('neg_neu_posteriors')\n",
    "\n",
    "test_data = [[pos_neg_posteriors['test'][i][0], pos_neg_posteriors['test'][i][1], pos_neu_posteriors['test'][i][0], pos_neu_posteriors['test'][i][1], neg_neu_posteriors['test'][i][0], neg_neu_posteriors['test'][i][1]] for i in range(len(pos_neg_posteriors['test']))]\n",
    "train_data = [[pos_neg_posteriors['train'][i][0], pos_neg_posteriors['train'][i][1], pos_neu_posteriors['train'][i][0], pos_neu_posteriors['train'][i][1], neg_neu_posteriors['train'][i][0], neg_neu_posteriors['train'][i][1]] for i in range(len(pos_neg_posteriors['train']))]\n",
    "\n",
    "scoreRR_KNN = knn_classification(train_data, test_data, y_train_labels, y_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ακρίβεια αλγορίθμων ταξινόμησης\n",
    "Παρακάτω παραθέτουμε το accuracy score του κάθε classifier που χρησιμοποιήσαμε, συναρτήσει των vectorizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame([('{:.1%}'.format(scoreBOW_KNN), '{:.1%}'.format(scoreTFIDF_KNN), '{:.1%}'.format(scoreW2V_KNN), '{:.1%}'.format(scoreW2Vplus_KNN), '{:.1%}'.format(scoreD2V_KNN), '{:.1%}'.format(scoreD2Vplus_KNN)),('{:.1%}'.format(scoreBOW_SVM), '{:.1%}'.format(scoreTFIDF_SVM), '{:.1%}'.format(scoreW2V_SVM), '{:.1%}'.format(scoreW2Vplus_SVM), '{:.1%}'.format(scoreD2V_SVM), '{:.1%}'.format(scoreD2Vplus_SVM)),('{:.1%}'.format(scoreRR_KNN),'','','','','')],\n",
    "                         index=['KNN', 'SVM', 'RR'], columns=['BOW', 'TFIDF', 'W2V', 'W2V+', 'D2V', 'D2V+'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Συμπεράσματα\n",
    "Προκειμένου να εξασφαλίσουμε ότι δε συμβαίνει overfitting στους classifiers, συγκρίναμε την ακρίβεια του ταξινομητή με είσοδο το train set και το test set αντίστοιχα. Παρατηρήσαμε ότι σε κάθε περίπτωση η διαφορά της ακρίβειας των αλγορίθμων για κάθε set, δεν υπερβαίνει τις 11 μονάδες.\n",
    "\n",
    "<img src=\"accuracy_table.png\" style = \"margin-bottom:50px\">\n",
    "\n",
    "\n",
    "* Ο SVM είναι πιο ακριβής από τον KNN, ανεξαρτήτως του αριθμού των γειτόνων (για τον KNN) και του vectorizer.\n",
    "* Ο SVM είναι γρηγορότερος από τον KNN.\n",
    "* Ο KNN με περισσότερους γείτονες(>10) είναι αποδοτικότερος απ' ότι με λιγότερους. Όσο αυξάνουμε τον αριθμό των γειτόνων, τόσο αυξάνεται και ο χρόνος εκτέλεσης του αλγορίθμου. Ωστόσο, δεν είναι ωφέλιμο να αυξήσουμε τον αριθμό των γειτόνων αρκετά, καθώς η ακρίβεια παραμένει σταθερή ή μειώνεται, ενώ ο αλγόριθμος εκτελείται για περισσότερο χρόνο.\n",
    "* Ο αλγόριθμος TF-IDF είναι πιο αποδοτικός από τον Bag of words καθώς τα διανύσματα που δημιουργεί αποτελούνται από δεκαδικά ψηφία και είναι πιο custom, ωστόσο απαιτεί περισσότερο χρόνο εκτέλεσης. \n",
    "* Ο εμπλουτισμός των embeddings με custom χαρακτηριστικά βελτιώνει σημαντικα την ακρίβεια των ταξινομητών (περίπου κατά 12%).\n",
    "* Δεν παρατηρήσαμε αισθητή διαφορά μεταξύ των vectorizers Word2Vec και Doc2Vec. Συγκεκριμένα, η ακρίβεια του Word2Vec στον KNN υπερτερεί ελάχιστα από αυτή του Doc2Vec, ενώ στον SVM συμβαίνει το αντίθετο. Πιστεύουμε ότι από το συγκεκριμένο dataset δεν μπορεί να βγει γενικό συμπέρασμα για το ποιος vectorizer είναι \"καλύτερος\".\n",
    "* Ακριβώς το ίδιο αποτέλεσμα παρατηρούμε και με τη χρήση των εμπλουτισμένων Word2Vec και Doc2Vec vectorizers, δίχως να καταλήγουμε σε σαφή συμπεράσματα.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
