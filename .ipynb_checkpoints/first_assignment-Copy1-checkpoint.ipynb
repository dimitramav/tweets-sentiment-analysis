{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "##### STRIP TWEET #####################################\n",
    "#######################################################\n",
    "\n",
    "from utils import clean_corpus\n",
    "import pandas as pd\n",
    "\n",
    "#READ TRAINING SET\n",
    "df = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "train_corpus = df[3].tolist()\n",
    "\n",
    "#READ TEST SET\n",
    "df = pd.read_csv(\"test2017.tsv\", sep='\\t', header=None)\n",
    "test_corpus = df[3].tolist()\n",
    "\n",
    "clean_train_corpus = clean_corpus(train_corpus)\n",
    "clean_test_corpus = clean_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "##### TOKENIZATION ####################################\n",
    "#######################################################\n",
    "\n",
    "from utils import tokenize, lemmatize\n",
    "\n",
    "train_tokens = tokenize(clean_train_corpus)\n",
    "test_tokens = tokenize(clean_test_corpus)\n",
    "\n",
    "train_tweets = lemmatize(train_tokens)\n",
    "test_tweets = lemmatize(test_tokens)\n",
    "\n",
    "final_train_corpus = [\" \".join(str(word) for word in tweet) for tweet in train_tweets]\n",
    "final_test_corpus = [\" \".join(str(word) for word in tweet) for tweet in test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAG-OF-WORDS VECTORIZATION\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import save_to_pickle\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_BOW_train = vectorizer.fit_transform(final_train_corpus)\n",
    "save_to_pickle('X_BOW_train',X_BOW_train)\n",
    "\n",
    "X_BOW_test = vectorizer.transform(final_test_corpus)\n",
    "save_to_pickle('X_BOW_test',X_BOW_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TF-IDF VECTORIZATION\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from utils import save_to_pickle\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_TFIDF_train = vectorizer.fit_transform(final_train_corpus)\n",
    "save_to_pickle('X_TFIDF_train',X_TFIDF_train)\n",
    "\n",
    "X_TFIDF_test = vectorizer.transform(final_test_corpus)\n",
    "save_to_pickle('X_TFIDF_test',X_TFIDF_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have 3 options: word2vec, word2vec in sklearn, doc2vec\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from utils import create_word_embeddings, save_to_pickle\n",
    "\n",
    "#train_tweets\n",
    "model_train = Word2Vec(train_tweets, size=200, window=5, min_count=1, workers=4) # size of vector is 200\n",
    "model_train.train(train_tweets, total_examples=model_train.corpus_count, epochs=model_train.epochs)  # train word vectors\n",
    "\n",
    "X_W2V_embeddings_train = create_word_embeddings(train_tweets, model_train)\n",
    "save_to_pickle('X_W2V_embeddings_train',X_W2V_embeddings_train)\n",
    "\n",
    "#test_tweets\n",
    "model_test = Word2Vec(test_tweets, size=200, window=5, min_count=1, workers=4) # size of vector is 200\n",
    "model_test.train(test_tweets, total_examples=model_test.corpus_count,epochs=model_test.epochs)  # train word vectors\n",
    "\n",
    "X_W2V_embeddings_test = create_word_embeddings(test_tweets, model_test)\n",
    "save_to_pickle('X_W2V_embeddings_test',X_W2V_embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doc2vec is equal with word2vec but is more appropriate for phrases (vectorize phrases instead of words). I.e \n",
    "#1.Manos leaves the office every day at 18:00 to catch his train\n",
    "#2. This season is called Fall, because leaves fall from the trees.\n",
    "#In this way we can capture the difference between the same word used in a different context. For example we now have a\n",
    "#different representation of the word “leaves” in the above two sentences\n",
    "\n",
    "from utils import create_doc_embeddings, save_to_pickle\n",
    "\n",
    "X_embeddings_array_train = create_doc_embeddings(final_train_corpus)\n",
    "X_embeddings_array_test = create_doc_embeddings(final_test_corpus)\n",
    "\n",
    "X_D2V_embeddings_train = [X_embeddings_array_train[i].tolist() for i in range(len(X_embeddings_array_train))]\n",
    "save_to_pickle('X_D2V_embeddings_train',X_D2V_embeddings_train)\n",
    "\n",
    "X_D2V_embeddings_test = [X_embeddings_array_test[i].tolist() for i in range(len(X_embeddings_array_test))]\n",
    "save_to_pickle('X_D2V_embeddings_test',X_D2V_embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_to_pickle, load_from_pickle, add_characteristics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "lexica_df = [pd.read_csv(\"lexica/affin/affin.txt\", sep='\\t', header=None),\n",
    "              pd.read_csv(\"lexica/emotweet/valence_tweet.txt\", sep='\\t', header=None),\n",
    "              pd.read_csv(\"lexica/generic/generic.txt\", sep='\\t', engine=\"python\" ,quoting=csv.QUOTE_NONE,header=None),\n",
    "              pd.read_csv(\"lexica/nrc/val.txt\", sep='\\t', engine=\"python\",quoting=csv.QUOTE_NONE, header=None),\n",
    "              pd.read_csv(\"lexica/nrctag/val.txt\", sep='\\t', header=None)]\n",
    "lexica = [df.set_index(0).T.to_dict('list') for df in lexica_df]\n",
    "\n",
    "characteristics_train = add_characteristics(lexica,train_tweets)\n",
    "characteristics_test = add_characteristics(lexica,test_tweets) \n",
    "\n",
    "X_W2V_embeddings_train = load_from_pickle('X_W2V_embeddings_train')\n",
    "X_W2Vplus_embeddings_train = np.concatenate((X_W2V_embeddings_train,characteristics_train), axis=1)\n",
    "save_to_pickle('X_W2Vplus_embeddings_train',X_W2Vplus_embeddings_train)\n",
    "\n",
    "X_W2V_embeddings_test = load_from_pickle('X_W2V_embeddings_test')\n",
    "X_W2Vplus_embeddings_test = np.concatenate((X_W2V_embeddings_test,characteristics_test), axis=1)\n",
    "save_to_pickle('X_W2Vplus_embeddings_test',X_W2Vplus_embeddings_test)\n",
    "\n",
    "X_D2V_embeddings_train = load_from_pickle('X_D2V_embeddings_train')\n",
    "X_D2Vplus_embeddings_train = np.concatenate((X_D2V_embeddings_train,characteristics_train), axis=1)\n",
    "save_to_pickle('X_D2Vplus_embeddings_train',X_D2Vplus_embeddings_train)\n",
    "\n",
    "X_D2V_embeddings_test = load_from_pickle('X_D2V_embeddings_test')\n",
    "X_D2Vplus_embeddings_test = np.concatenate((X_D2V_embeddings_test,characteristics_test), axis=1)\n",
    "save_to_pickle('X_D2Vplus_embeddings_test',X_D2Vplus_embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### CONSTRUCT Y_LABELS ###\n",
    "##########################\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "y_train_labels = df[2].tolist() #sentiments\n",
    "\n",
    "for n, value in enumerate(y_train_labels):\n",
    "    if value == \"positive\":\n",
    "        y_train_labels[n] = 2\n",
    "    elif value ==\"negative\":\n",
    "        y_train_labels[n] = 0\n",
    "    else:\n",
    "        y_train_labels[n] = 1\n",
    "\n",
    "df = pd.read_csv(\"y_test_labels.tsv\", sep='\\t', header=None)\n",
    "y_test_labels = df[1].tolist() #sentiments\n",
    "\n",
    "for n, value in enumerate(y_test_labels):\n",
    "    if value == \"positive\":\n",
    "        y_test_labels[n] = 2\n",
    "    elif value ==\"negative\":\n",
    "        y_test_labels[n] = 0\n",
    "    else:\n",
    "        y_test_labels[n] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_BOW_train')\n",
    "X_test = load_from_pickle('X_BOW_test')\n",
    "\n",
    "#BOW - KNN CLASSIFICATION\n",
    "scoreBOW_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#BOW - SVM CLASSIFICATION\n",
    "scoreBOW_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreBOW_KNN, scoreBOW_SVM) #na metaferw ta scores se ena megalo pinaka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_TFIDF_train')\n",
    "X_test = load_from_pickle('X_TFIDF_test')\n",
    "\n",
    "#TFIDF - KNN CLASSIFICATION\n",
    "scoreTFIDF_KNN = knn_classification(X_TFIDF_train, X_TFIDF_test, y_train_labels, y_test_labels)\n",
    "#TFIDF - SVM CLASSIFICATION\n",
    "scoreTFIDF_SVM = svm_classification(X_TFIDF_train, X_TFIDF_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreTFIDF_KNN, scoreTFIDF_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_D2V_embeddings_train')\n",
    "X_test = load_from_pickle('X_D2V_embeddings_test')\n",
    "\n",
    "#DOC2VEC - KNN CLASSIFICATION\n",
    "scoreD2V_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#DOC2VEC - SVM CLASSIFICATION\n",
    "scoreD2V_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreD2V_KNN, scoreD2V_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_D2Vplus_embeddings_train')\n",
    "X_test = load_from_pickle('X_D2Vplus_embeddings_test')\n",
    "\n",
    "#DOC2VEC+features - KNN CLASSIFICATION\n",
    "scoreD2Vplus_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#DOC2VEC+features - SVM CLASSIFICATION\n",
    "scoreD2Vplus_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreD2Vplus_KNN, scoreD2Vplus_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_W2V_embeddings_train')\n",
    "X_test = load_from_pickle('X_W2V_embeddings_test')\n",
    "\n",
    "#WORD2VEC - KNN CLASSIFICATION\n",
    "scoreW2V_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#WORD2VEC - SVM CLASSIFICATION\n",
    "scoreW2V_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreW2V_KNN, scoreW2V_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_from_pickle, knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_W2Vplus_embeddings_train')\n",
    "X_test = load_from_pickle('X_W2Vplus_embeddings_test')\n",
    "\n",
    "#WORD2VEC+features - KNN CLASSIFICATION\n",
    "scoreW2Vplus_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#WORD2VEC+features - SVM CLASSIFICATION\n",
    "scoreW2Vplus_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreW2Vplus_KNN, scoreW2Vplus_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorization = ['BOW', 'BOW', 'TDIF', 'TDIF', 'D2V', 'D2V', 'D2V+', 'D2V+', 'W2V', 'W2V', 'W2V+', 'W2V+']\n",
    "#classifiers = ['KNN', 'SVM', 'KNN', 'SVM', 'KNN', 'SVM', 'KNN', 'SVM', 'KNN', 'SVM', 'KNN', 'SVM']\n",
    "#accuracy_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ioannis/anaconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "###ROUND ROBIN###\n",
    "#################\n",
    "\n",
    "from utils import create_posteriors, knn_classification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "train_tweets = df_train[3].tolist()\n",
    "df_test = pd.read_csv(\"test2017.tsv\", sep='\\t', header=None)\n",
    "test_tweets = df_test[3].tolist()\n",
    "\n",
    "sent_map = {\"positive\":2, \"neutral\":1, \"negative\":0}\n",
    "\n",
    "pos_neg_train = df_train.loc[(df_train[2] == \"positive\") | (df_train[2] == \"negative\")].copy()\n",
    "pos_neg_labels = [sent_map[sentiment] for sentiment in pos_neg_train[2].tolist()] #sentiments\n",
    "pos_neg_posteriors = create_posteriors(pos_neg_train[3].tolist(),train_tweets,test_tweets,pos_neg_labels,1)\n",
    "\n",
    "pos_neu_train = df_train.loc[(df_train[2] == \"positive\") | (df_train[2] == \"neutral\")].copy()\n",
    "pos_neu_labels = [sent_map[sentiment] for sentiment in pos_neu_train[2].tolist()]\n",
    "pos_neu_posteriors = create_posteriors(pos_neu_train[3].tolist(),train_tweets,test_tweets,pos_neu_labels,1)\n",
    "\n",
    "neg_neu_train = df_train.loc[(df_train[2] == \"negative\") | (df_train[2] == \"neutral\")].copy()\n",
    "neg_neu_labels = [sent_map[sentiment] for sentiment in neg_neu_train[2].tolist()]\n",
    "neg_neu_posteriors = create_posteriors(neg_neu_train[3].tolist(),train_tweets,test_tweets,neg_neu_labels,1)\n",
    "\n",
    "#na apothikeusw tis domes se pickle arxeio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45791273200911753\n"
     ]
    }
   ],
   "source": [
    "from utils import knn_classification\n",
    "\n",
    "test_data = [[pos_neg_posteriors['test'][i][0], pos_neg_posteriors['test'][i][1], pos_neu_posteriors['test'][i][0], pos_neu_posteriors['test'][i][1], neg_neu_posteriors['test'][i][0], neg_neu_posteriors['test'][i][1]] for i in range(len(pos_neg_posteriors['test']))]\n",
    "train_data = [[pos_neg_posteriors['train'][i][0], pos_neg_posteriors['train'][i][1], pos_neu_posteriors['train'][i][0], pos_neu_posteriors['train'][i][1], neg_neu_posteriors['train'][i][0], neg_neu_posteriors['train'][i][1]] for i in range(len(pos_neg_posteriors['train']))]\n",
    "\n",
    "scoreRR_KNN = knn_classification(train_data, test_data, y_train_labels, y_test_labels)\n",
    "print(scoreRR_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
