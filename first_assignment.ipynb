{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "##### STRIP TWEET #####################################\n",
    "#######################################################\n",
    "\n",
    "from utils import clean_corpus\n",
    "import pandas as pd\n",
    "\n",
    "#READ TRAINING SET\n",
    "df = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "train_corpus = df[3].tolist()\n",
    "\n",
    "#READ TEST SET\n",
    "df = pd.read_csv(\"test2017.tsv\", sep='\\t', header=None)\n",
    "test_corpus = df[3].tolist()\n",
    "\n",
    "clean_train_corpus = clean_corpus(train_corpus)\n",
    "clean_test_corpus = clean_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "##### TOKENIZATION ####################################\n",
    "#######################################################\n",
    "\n",
    "from utils import tokenize, lemmatize\n",
    "\n",
    "train_tokens = tokenize(clean_train_corpus)\n",
    "test_tokens = tokenize(clean_test_corpus)\n",
    "\n",
    "train_tweets = lemmatize(train_tokens)\n",
    "test_tweets = lemmatize(test_tokens)\n",
    "\n",
    "final_train_corpus = [\" \".join(str(word) for word in tweet) for tweet in train_tweets]\n",
    "final_test_corpus = [\" \".join(str(word) for word in tweet) for tweet in test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAG-OF-WORDS VECTORIZATION\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import save_to_pickle\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_BOW_train = vectorizer.fit_transform(final_train_corpus)\n",
    "save_to_pickle('X_BOW_train',X_BOW_train)\n",
    "\n",
    "X_BOW_test = vectorizer.transform(final_test_corpus)\n",
    "save_to_pickle('X_BOW_test',X_BOW_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TF-IDF VECTORIZATION\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_TFIDF_train = vectorizer.fit_transform(final_train_corpus)\n",
    "save_to_pickle('X_TFIDF_train',X_TFIDF_train)\n",
    "\n",
    "X_TFIDF_test = vectorizer.transform(final_test_corpus)\n",
    "save_to_pickle('X_TFIDF_test',X_TFIDF_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have 3 options: word2vec, word2vec in sklearn, doc2vec\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from utils import create_word_embeddings\n",
    "\n",
    "#train_tweets\n",
    "model_train = Word2Vec(train_tweets, size=200, window=5, min_count=1, workers=4) # size of vector is 200\n",
    "model_train.train(train_tweets, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
    "\n",
    "X_W2V_embeddings_train = create_word_embeddings(train_tweets, model_train)\n",
    "save_to_pickle('X_W2V_embeddings_train',X_W2V_embeddings_train)\n",
    "\n",
    "#test_tweets\n",
    "model_test = Word2Vec(test_tweets, size=200, window=5, min_count=1, workers=4) # size of vector is 200\n",
    "model_test.train(test_tweets, total_examples=model.corpus_count,epochs=model.epochs)  # train word vectors\n",
    "\n",
    "X_W2V_embeddings_test = create_word_embeddings(test_tweets, model_test)\n",
    "save_to_pickle('X_W2V_embeddings_test',X_W2V_embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doc2vec is equal with word2vec but is more appropriate for phrases (vectorize phrases instead of words). I.e \n",
    "#1.Manos leaves the office every day at 18:00 to catch his train\n",
    "#2. This season is called Fall, because leaves fall from the trees.\n",
    "#In this way we can capture the difference between the same word used in a different context. For example we now have a\n",
    "#different representation of the word “leaves” in the above two sentences\n",
    "\n",
    "#IMPORTANT: parameteres in my case is random, we have to pay attention to select the right ones\n",
    "\n",
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def word_embeddings(final_corpus):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(final_corpus)]\n",
    "    print(tagged_data)\n",
    "    model = Doc2Vec(size=200,\n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "    model.build_vocab(tagged_data)\n",
    "    max_epoch = 20\n",
    "    for epoch in range(max_epoch):\n",
    "        model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    X_embeddings_array = model.docvecs\n",
    "    return X_embeddings_array\n",
    "\n",
    "#convert array to list\n",
    "\n",
    "X_embeddings_array_train = word_embeddings(final_train_corpus)\n",
    "X_embeddings_array_test = word_embeddings(final_test_corpus)\n",
    "\n",
    "X_D2V_embeddings_train=[]\n",
    "for i in range(len(X_embeddings_array_train)):\n",
    "    X_D2V_embeddings_train.append(X_embeddings_array_train[i].tolist())\n",
    "    \n",
    "with open('X_D2V_embeddings_train.pickle','wb') as handle: \n",
    "    pickle.dump(X_D2V_embeddings_train,handle,protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "X_D2V_embeddings_test=[]\n",
    "for i in range(len(X_embeddings_array_test)):\n",
    "    X_D2V_embeddings_test.append(X_embeddings_array_test[i].tolist())\n",
    "    \n",
    "with open('X_D2V_embeddings_test.pickle','wb') as handle: \n",
    "    pickle.dump(X_D2V_embeddings_test,handle,protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add characteristics to embeddings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from statistics import mean\n",
    "\n",
    "lexica_df =[pd.read_csv(\"lexica/affin/affin.txt\", sep='\\t', header=None),\n",
    "              pd.read_csv(\"lexica/emotweet/valence_tweet.txt\", sep='\\t', header=None),\n",
    "              pd.read_csv(\"lexica/generic/generic.txt\", sep='\\t', engine=\"python\" ,quoting=csv.QUOTE_NONE,header=None),\n",
    "              pd.read_csv(\"lexica/nrc/val.txt\", sep='\\t', engine=\"python\",quoting=csv.QUOTE_NONE, header=None),\n",
    "              pd.read_csv(\"lexica/nrctag/val.txt\", sep='\\t', header=None)]\n",
    "lexica = [df.set_index(0).T.to_dict('list') for df in lexica_df]\n",
    "\n",
    "def add_characteristics(tweets):\n",
    "    characteristics = [[] for i in range(len(tweets))]\n",
    "    for tweet in range(len(tweets)):   #for each tweet\n",
    "        #characteristics[tweet].append(sum([len(token) for token in tweets[tweet]])) #length of each tweet\n",
    "        characteristics[tweet].append(len(tweets[tweet])) #length of each tweet\n",
    "        for lexicon in lexica:\n",
    "            tweet_sentiments = [lexicon.get(token,[0])[0] for token in tweets[tweet]]\n",
    "            max_valence = max(tweet_sentiments)\n",
    "            min_valence = min(tweet_sentiments)\n",
    "            average = mean(tweet_sentiments)\n",
    "            if len(tweet_sentiments) > 1:\n",
    "                average_half1 = mean(tweet_sentiments[:len(tweet_sentiments)//2])\n",
    "                average_half2 = mean(tweet_sentiments[len(tweet_sentiments)//2:])\n",
    "            else:\n",
    "                average_half1 = len(tweet_sentiments)\n",
    "                average_half2 = 0\n",
    "            characteristics[tweet].extend((max_valence, min_valence, average, average_half1, average_half2))\n",
    "    print(tweet) #gia na doyme proodo!\n",
    "\n",
    "    return characteristics\n",
    "\n",
    "characteristics_train = add_characteristics(train_tweets)\n",
    "print(\"next one\") #gia na dw an pige sto epomeno set!\n",
    "characteristics_test = add_characteristics(test_tweets) \n",
    "\n",
    "with open('X_W2V_embeddings_train.pickle','rb') as handle: \n",
    "    X_W2V_embeddings_train = pickle.load(handle)\n",
    "    X_W2Vplus_embeddings_train = np.concatenate((X_W2V_embeddings_train,characteristics_train), axis=1)\n",
    "    with open('X_W2Vplus_embeddings_train.pickle','wb') as handle: \n",
    "        pickle.dump(X_W2Vplus_embeddings_train,handle,protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('X_W2V_embeddings_test.pickle','rb') as handle: \n",
    "    X_W2V_embeddings_test = pickle.load(handle)\n",
    "    X_W2Vplus_embeddings_test = np.concatenate((X_W2V_embeddings_test,characteristics_test), axis=1)\n",
    "    with open('X_W2Vplus_embeddings_test.pickle','wb') as handle: \n",
    "        pickle.dump(X_W2Vplus_embeddings_test,handle,protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "with open('X_D2V_embeddings_train.pickle','rb') as handle: \n",
    "    X_D2V_embeddings_train = pickle.load(handle)\n",
    "    X_D2Vplus_embeddings_train = np.concatenate((X_D2V_embeddings_train,characteristics_train), axis=1)\n",
    "    with open('X_D2Vplus_embeddings_train.pickle','wb') as handle: \n",
    "        pickle.dump(X_D2Vplus_embeddings_train,handle,protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('X_D2V_embeddings_test.pickle','rb') as handle: \n",
    "    X_D2V_embeddings_test = pickle.load(handle)\n",
    "    X_D2Vplus_embeddings_test = np.concatenate((X_D2V_embeddings_test,characteristics_test), axis=1)\n",
    "    with open('X_D2Vplus_embeddings_test.pickle','wb') as handle: \n",
    "        pickle.dump(X_D2Vplus_embeddings_test,handle,protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#CONSTRUCT Y_LABELS#\n",
    "####################\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"train2017.tsv\", sep='\\t', header=None)\n",
    "y_train_labels = df[2].tolist() #sentiments\n",
    "\n",
    "\n",
    "for n, value in enumerate(y_train_labels):\n",
    "    if value == \"positive\":\n",
    "        y_train_labels[n] = 2\n",
    "    elif value ==\"negative\":\n",
    "        y_train_labels[n] = 0\n",
    "    else:\n",
    "        y_train_labels[n] =1\n",
    "\n",
    "df = pd.read_csv(\"y_test_labels.tsv\", sep='\\t', header=None)\n",
    "y_test_labels = df[1].tolist() #sentiments\n",
    "\n",
    "\n",
    "for n, value in enumerate(y_test_labels):\n",
    "    if value == \"positive\":\n",
    "        y_test_labels[n] = 2\n",
    "    elif value ==\"negative\":\n",
    "        y_test_labels[n] = 0\n",
    "    else:\n",
    "        y_test_labels[n] =1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_BOW_train')\n",
    "X_test = load_from_pickle('X_BOW_test')\n",
    "\n",
    "#BOW - KNN CLASSIFICATION\n",
    "scoreBOW_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#BOW - SVM CLASSIFICATION\n",
    "scoreBOW_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreBOW_KNN, scoreBOW_SVM) #na metaferw ta scores se ena megalo pinaka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_TFIDF_train')\n",
    "X_test = load_from_pickle('X_TFIDF_test')\n",
    "\n",
    "#TFIDF - KNN CLASSIFICATION\n",
    "scoreTFIDF_KNN = knn_classification(X_TFIDF_train, X_TFIDF_test, y_train_labels, y_test_labels)\n",
    "#TFIDF - SVM CLASSIFICATION\n",
    "scoreTFIDF_SVM = svm_classification(X_TFIDF_train, X_TFIDF_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreTFIDF_KNN, scoreTFIDF_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_D2V_embeddings_train')\n",
    "X_test = load_from_pickle('X_D2V_embeddings_test')\n",
    "\n",
    "#DOC2VEC - KNN CLASSIFICATION\n",
    "scoreD2V_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#DOC2VEC - SVM CLASSIFICATION\n",
    "scoreD2V_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreD2V_KNN, scoreD2V_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_D2Vplus_embeddings_train')\n",
    "X_test = load_from_pickle('X_D2Vplus_embeddings_test')\n",
    "\n",
    "#DOC2VEC+features - KNN CLASSIFICATION\n",
    "scoreD2Vplus_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#DOC2VEC+features - SVM CLASSIFICATION\n",
    "scoreD2Vplus_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreD2Vplus_KNN, scoreD2Vplus_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_W2V_embeddings_train')\n",
    "X_test = load_from_pickle('X_W2V_embeddings_test')\n",
    "\n",
    "#WORD2VEC - KNN CLASSIFICATION\n",
    "scoreW2V_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#WORD2VEC - SVM CLASSIFICATION\n",
    "scoreW2V_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreW2V_KNN, scoreW2V_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import knn_classification, svm_classification\n",
    "\n",
    "X_train = load_from_pickle('X_W2Vplus_embeddings_train')\n",
    "X_test = load_from_pickle('X_W2Vplus_embeddings_test')\n",
    "\n",
    "#WORD2VEC+features - KNN CLASSIFICATION\n",
    "scoreW2Vplus_KNN = knn_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "#WORD2VEC+features - SVM CLASSIFICATION\n",
    "scoreW2Vplus_SVM = svm_classification(X_train, X_test, y_train_labels, y_test_labels)\n",
    "\n",
    "print(scoreW2Vplus_KNN, scoreW2Vplus_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#ROUND ROBIN#\n",
    "#############\n",
    "\n",
    "######################\n",
    "#CONSTRUCT TRAIN_SETS#\n",
    "######################\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "\n",
    "# df = pd.read_csv(\"small_train.tsv\", sep='\\t', header=None)\n",
    "# y_train_labels = df[2].tolist() #sentiments\n",
    "# train_tweets = df[3].tolist() #tweets\n",
    "\n",
    "# df = pd.read_csv(\"small_test.tsv\", sep='\\t', header=None)\n",
    "# test_tweets = df[3].tolist() #tweets\n",
    "\n",
    "# pos_neg_trainset = pd.DataFrame()\n",
    "# pos_neg_trainset = df.loc[(df[2] == \"positive\") | (df[2] == \"negative\")].copy()\n",
    "# clean_pos_neg_corpus = clean_corpus(pos_neg_trainset[3].tolist())\n",
    "# train_pos_neg_tokens = tokenize(clean_pos_neg_corpus)\n",
    "# train_pos_neg_tweets = lemmatize(train_pos_neg_tokens)\n",
    "\n",
    "\n",
    "# pos_neu_trainset = pd.DataFrame()\n",
    "# pos_neu_trainset = df.loc[(df[2] == \"positive\") | (df[2] == \"neutral\")].copy()\n",
    "# clean_pos_neu_corpus = clean_corpus(pos_neg_trainset[3].tolist())\n",
    "# train_pos_neu_tokens = tokenize(clean_pos_neu_corpus)\n",
    "# train_pos_neu_tweets = lemmatize(train_pos_neu_tokens)\n",
    "\n",
    "# neg_neu_trainset = pd.DataFrame()\n",
    "# neg_neu_trainset = df.loc[(df[2] == \"negative\") | (df[2] == \"neutral\")].copy()\n",
    "# clean_neg_neu_corpus = clean_corpus(pos_neg_trainset[3].tolist())\n",
    "# train_neg_neu_tokens = tokenize(clean_neg_neu_corpus)\n",
    "# train_neg_neu_tweets = lemmatize(train_neg_neu_tokens)\n",
    "\n",
    "\n",
    "#         knn = KNeighborsClassifier(n_neighbors=10)\n",
    "#         knn.fit(train_neg_neu_tweets, neg_neu_trainset[2].tolist())\n",
    "#         y_pred_train = knn.predict(train_neg_neu_tweets)  #predict proba\n",
    "#         y_pred_test = knn.predict(test_tweets) #predict proba\n",
    "#         print(metrics.accuracy_score(y_test_labels, y_pred))\n",
    "#==================================================================================\n",
    "# for n, value in enumerate(y_train_labels):\n",
    "#     if value == \"positive\":\n",
    "#         y_train_labels[n] = 2\n",
    "        \n",
    "#     elif value ==\"negative\":\n",
    "#         y_train_labels[n] = 0\n",
    "#     else:\n",
    "#         y_train_labels[n] =1\n",
    "\n",
    "# df = pd.read_csv(\"y_test_labels.tsv\", sep='\\t', header=None)\n",
    "# y_test_labels = df[1].tolist() #sentiments\n",
    "\n",
    "\n",
    "# for n, value in enumerate(y_test_labels):\n",
    "#     if value == \"positive\":\n",
    "#         y_test_labels[n] = 2\n",
    "#     elif value ==\"negative\":\n",
    "#         y_test_labels[n] = 0\n",
    "#     else:\n",
    "#         y_test_labels[n] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
